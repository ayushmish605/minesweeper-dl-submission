{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03 — Task 2 (Playing by Move Prediction)\n",
        "\n",
        "This notebook is my implementation of **Task 2**.\n",
        "\n",
        "The PDF describes Task 2 for **medium difficulty**, but I run the same actor/critic setup for:\n",
        "\n",
        "- Easy (22×22, 50 mines)\n",
        "- Medium (22×22, 80 mines)\n",
        "- Hard (22×22, 100 mines)\n",
        "\n",
        "## What my critic predicts\n",
        "For a given visible board state and a candidate click (row, col), my critic predicts a **survival score** in **[0, 1]**.\n",
        "\n",
        "I define survival for a full playthrough as:\n",
        "\n",
        "- Let **T** = total number of clicks taken until the game reaches 100% progress (all safe cells opened), i.e. `WON` if 0 mines were triggered or `DONE` if >=1 mine was triggered.\n",
        "- Let **F** = the step index (1-indexed) of the **first** mine trigger.\n",
        "  - If no mine ever triggers, I set **F = T**.\n",
        "- Then `survival` = F / T.\n",
        "\n",
        "So:\n",
        "- survival = 1.0 means the first mine (if any) happened only at the end (and in the perfect case: no mines were triggered at all).\n",
        "- survival = 0.6 means the first mine happened at step 60 of a 100-click completion.\n",
        "\n",
        "When I report `avg_survival`, it is the **mean of this per-game survival score** over the evaluation episodes.\n",
        "\n",
        "## What I optimize in the actor\n",
        "- If LogicBot has any provably-safe moves, I use the network to **choose the best safe move** (this is where the model can pick a higher-survival safe click than the LogicBot’s default choice).\n",
        "- Only when LogicBot is forced to guess do I use the network to choose among guess candidates.\n",
        "\n",
        "For any candidate (row, col), I score it with:\n",
        "- score = predicted_survival - mine_penalty * P(mine)\n",
        "\n",
        "## Metrics I report\n",
        "- `perfect_win_rate`: fraction of games that finish with 100% progress **and** `mines_triggered == 0`.\n",
        "- `avg_survival`: average of \\(F/T\\) described above.\n",
        "\n",
        "Assumption: I already unzipped my project so the repo lives at `/content/repo/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab installs\n",
        "# NOTE: I avoid re-installing torch in Colab because it can create checkpoint loading issues\n",
        "# if the runtime's torch version changes mid-session.\n",
        "%pip install -q numpy tqdm matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Repo root\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path('/content/repo')\n",
        "\n",
        "# Sometimes zip extraction creates one extra top-level folder; if so, I step into it.\n",
        "if not ((repo_root / 'minesweeper').exists() and (repo_root / 'models').exists()):\n",
        "    kids = [p for p in repo_root.iterdir() if p.is_dir()]\n",
        "    if len(kids) == 1:\n",
        "        repo_root = kids[0]\n",
        "\n",
        "if not ((repo_root / 'minesweeper').exists() and (repo_root / 'models').exists()):\n",
        "    raise FileNotFoundError(f'Bad repo_root: {repo_root}')\n",
        "\n",
        "sys.path.insert(0, str(repo_root))\n",
        "print('Repo root:', repo_root)\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1 — Task 2 data + policy utilities\n",
        "#\n",
        "# The Task 2 instructions describe two phases:\n",
        "# 1) Learn to predict a survival score in [0,1] for a candidate click.\n",
        "#    I define survival as (step of first mine trigger) / (total steps to reach 100% progress).\n",
        "# 2) Use that network as an Actor on forced-guess states (when LogicBot has no safe move).\n",
        "#    Then train a Critic on the Actor's own guesses and bootstrap.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from minesweeper.game import GameState, MinesweeperGame\n",
        "from minesweeper.logic_bot import LogicBot\n",
        "from models.task1.encoding import visible_to_int8\n",
        "from models.task2.dataset import _clone_game_fast\n",
        "from models.task2.policy import (\n",
        "    actor_choose_click_value_map as _actor_choose_click_value_map,\n",
        "    allowed_coords_from_logic as _allowed_coords_from_logic,\n",
        "    logic_infer_sets as _logic_infer_sets,\n",
        ")\n",
        "from models.task2.value_map_model import BoardValuePredictor, BoardValuePredictorConfig\n",
        "\n",
        "# I keep evaluation deterministic and use exploration only during collection.\n",
        "EPS_COLLECT = 0.05\n",
        "TOPK_COLLECT = 5\n",
        "EPS_EVAL = 0.0\n",
        "TOPK_EVAL = 1\n",
        "\n",
        "# I default to scoring ALL candidates (\"every possible move\"), but keep this knob for speed.\n",
        "MAX_CANDIDATES = None  # set to e.g. 128 if you need more speed\n",
        "\n",
        "# Optional stability: use LogicBot inference as a mask.\n",
        "USE_LOGIC_MASK = True\n",
        "\n",
        "# Difficulty-aware choice: on easy, I keep the actor \"guess-only\" (never override LogicBot safe moves).\n",
        "# On medium/hard, I let the model rank safe moves too.\n",
        "USE_MODEL_ON_SAFE_MOVES = True\n",
        "\n",
        "# I combine value + mine-prob for action selection: score = value - mine_penalty * P(mine).\n",
        "# I keep mine_penalty a bit stronger on hard to reduce catastrophic guess failures.\n",
        "MINE_PENALTY = 4.0\n",
        "\n",
        "DIFFICULTIES = {\n",
        "    'easy': {'height': 22, 'width': 22, 'num_mines': 50},\n",
        "    'medium': {'height': 22, 'width': 22, 'num_mines': 80},\n",
        "    'hard': {'height': 22, 'width': 22, 'num_mines': 100},\n",
        "}\n",
        "\n",
        "DATA_DIR = Path(repo_root) / 'models' / 'task2' / 'datasets'\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CKPT_DIR = Path(repo_root) / 'models' / 'task2' / 'checkpoints'\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Note: I keep LogicBot inference + actor move-selection in `models/task2/policy.py`.\n",
        "# (Imported above as `_logic_infer_sets`, `_allowed_coords_from_logic`, and `_actor_choose_click_value_map`.)\n",
        "\n",
        "@torch.no_grad()\n",
        "def actor_choose_click(*, model: BoardValuePredictor, game: MinesweeperGame, bot: LogicBot, seed: int, epsilon: float, top_k: int):\n",
        "    return _actor_choose_click_value_map(\n",
        "        model=model,\n",
        "        game=game,\n",
        "        bot=bot,\n",
        "        device=device,\n",
        "        seed=int(seed),\n",
        "        mine_penalty=float(MINE_PENALTY),\n",
        "        epsilon=float(epsilon),\n",
        "        top_k=int(top_k),\n",
        "        use_logic_mask=bool(USE_LOGIC_MASK),\n",
        "        use_model_on_safe_moves=bool(USE_MODEL_ON_SAFE_MOVES),\n",
        "    )\n",
        "\n",
        "\n",
        "def collect_dataset_for_policy(\n",
        "    *,\n",
        "    diff: dict,\n",
        "    rollout_policy: str,\n",
        "    model: BoardValuePredictor | None,\n",
        "    num_games: int,\n",
        "    seed: int,\n",
        "    max_steps: int = 512,\n",
        "    states_per_game: int = 24,\n",
        "    record_prob: float = 0.25,\n",
        "    actions_per_state: int = 2,\n",
        "    target_samples: int | None = None,\n",
        "    max_games: int | None = None,\n",
        ") -> dict:\n",
        "    \"\"\"Collect sparse (s,a)->(survival_ratio, mine_clicked) supervision.\n",
        "\n",
        "    Forced-guess samples can be rare, so for actor rounds I sometimes want to collect until I\n",
        "    hit a target sample count.\n",
        "\n",
        "    - If target_samples is None: I collect for exactly num_games episodes.\n",
        "    - If target_samples is set: I keep playing until I reach that many samples, up to max_games.\n",
        "      (If max_games is None, I use a conservative safety cap.)\n",
        "    \"\"\"\n",
        "    rng = random.Random(int(seed))\n",
        "\n",
        "    h = int(diff['height'])\n",
        "    w = int(diff['width'])\n",
        "    m = int(diff['num_mines'])\n",
        "\n",
        "    xs = []\n",
        "    ars = []\n",
        "    y_survival = []\n",
        "    y_mine = []\n",
        "    ep_ids = []\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    last_print = t0\n",
        "    tgt = None\n",
        "    try:\n",
        "        tgt = int(target_samples) if target_samples is not None else None\n",
        "    except Exception:\n",
        "        tgt = None\n",
        "\n",
        "    # If I'm targeting a sample count, I may need more than num_games episodes.\n",
        "    # I keep a cap so collection can't run forever.\n",
        "    if tgt is None:\n",
        "        games_cap = int(num_games)\n",
        "    else:\n",
        "        if max_games is None:\n",
        "            games_cap = max(int(num_games), int(num_games) * 10)\n",
        "        else:\n",
        "            games_cap = max(int(num_games), int(max_games))\n",
        "\n",
        "    print(\n",
        "        f\"[dataset] start | mines={m} games<={int(games_cap)} target_samples={tgt} max_steps={int(max_steps)} \"\n",
        "        f\"states/game~{int(states_per_game)} record_prob={float(record_prob):.3f} actions/state={int(actions_per_state)} \"\n",
        "        f\"policy={str(rollout_policy)} eps_collect={float(EPS_COLLECT):.3f} topk_collect={int(TOPK_COLLECT)} mask={bool(USE_LOGIC_MASK)}\"\n",
        "    )\n",
        "\n",
        "    def _rollout_step(g: MinesweeperGame, bot: LogicBot, *, policy: str, model: BoardValuePredictor | None, seed_local: int, step_i: int):\n",
        "        kind = str(policy).strip().lower()\n",
        "        if kind == 'logic':\n",
        "            _logic_infer_sets(bot)\n",
        "            a = bot.select_action()\n",
        "            return None if a is None else (int(a[0]), int(a[1]))\n",
        "        if model is None:\n",
        "            raise ValueError('rollout_policy!=logic needs a model')\n",
        "        return actor_choose_click(model=model, game=g, bot=bot, seed=int(seed_local) + int(step_i), epsilon=float(EPS_COLLECT), top_k=int(TOPK_COLLECT))\n",
        "\n",
        "    def _rollout_to_end(\n",
        "        g2: MinesweeperGame,\n",
        "        *,\n",
        "        policy: str,\n",
        "        model2: BoardValuePredictor | None,\n",
        "        seed_local: int,\n",
        "        max_extra_steps: int,\n",
        "        steps0: int = 0,\n",
        "        first_mine_at0: int | None = None,\n",
        "    ):\n",
        "        \"\"\"Roll forward until DONE/WON or step cap, even if mines are triggered.\n",
        "\n",
        "        This returns metrics over the *entire* trajectory length, including any already-taken\n",
        "        prefix of length steps0.\n",
        "        \"\"\"\n",
        "        bot2 = LogicBot(g2, seed=int(seed_local))\n",
        "        steps2 = 0\n",
        "        first_mine_at = int(first_mine_at0) if first_mine_at0 is not None else None\n",
        "\n",
        "        while steps2 < int(max_extra_steps):\n",
        "            gs = g2.get_game_state()\n",
        "            if gs == GameState.PROG:\n",
        "                pass\n",
        "            else:\n",
        "                # Continue-after-mine mode keeps state LOST while still allowing more clicks.\n",
        "                if not (bool(getattr(g2, 'allow_mine_triggers', False)) and gs == GameState.LOST):\n",
        "                    break\n",
        "\n",
        "            a2 = _rollout_step(g2, bot2, policy=policy, model=model2, seed_local=seed_local, step_i=steps2)\n",
        "            if a2 is None:\n",
        "                break\n",
        "\n",
        "            prev_mines = int(getattr(g2, 'mines_triggered', 0) or 0)\n",
        "            steps2 += 1\n",
        "            _ = g2.player_clicks(int(a2[0]), int(a2[1]), set())\n",
        "            cur_mines = int(getattr(g2, 'mines_triggered', 0) or 0)\n",
        "            if first_mine_at is None and prev_mines == 0 and cur_mines > 0:\n",
        "                first_mine_at = int(steps0) + int(steps2)\n",
        "\n",
        "        total_steps = int(steps0) + int(steps2)\n",
        "        if total_steps <= 0:\n",
        "            return (0, 0.0)\n",
        "\n",
        "        fm = int(first_mine_at) if first_mine_at is not None else int(total_steps)\n",
        "        survival = float(fm) / float(total_steps)\n",
        "        return (total_steps, float(survival))\n",
        "\n",
        "    states_cap = max(1, int(states_per_game))\n",
        "    rec_p = float(record_prob)\n",
        "\n",
        "    for ep in range(int(games_cap)):\n",
        "        if tgt is not None and int(len(xs)) >= int(tgt):\n",
        "            break\n",
        "        now = time.perf_counter()\n",
        "        if (ep == 0) or ((now - last_print) > 6.0):\n",
        "            sps = float(len(xs) / max(1e-6, (now - t0)))\n",
        "            print(f\"[dataset] ep {ep}/{int(games_cap)} | samples={len(xs)} ({sps:.1f} samples/s)\")\n",
        "            last_print = now\n",
        "\n",
        "        game_seed = rng.randint(0, 2**31 - 1)\n",
        "        g = MinesweeperGame(height=h, width=w, num_mines=m, seed=int(game_seed))\n",
        "        # I continue after mines trigger so I can measure \"when the first mine happens\" vs full completion.\n",
        "        setattr(g, 'allow_mine_triggers', True)\n",
        "\n",
        "        first = (rng.randrange(h), rng.randrange(w))\n",
        "        g.player_clicks(int(first[0]), int(first[1]), set())\n",
        "\n",
        "        bot = LogicBot(g, seed=int(game_seed))\n",
        "\n",
        "        steps = 0\n",
        "        recorded = 0\n",
        "        while steps < int(max_steps):\n",
        "            gs = g.get_game_state()\n",
        "            if gs == GameState.PROG:\n",
        "                pass\n",
        "            else:\n",
        "                if not (bool(getattr(g, 'allow_mine_triggers', False)) and gs == GameState.LOST):\n",
        "                    break\n",
        "\n",
        "            visible = g.get_visible_board()\n",
        "\n",
        "            safe_coords = None\n",
        "            guess_coords = None\n",
        "            if bool(USE_LOGIC_MASK):\n",
        "                safe_coords, guess_coords = _allowed_coords_from_logic(bot, g)\n",
        "            forced_guess = (safe_coords is None) or (int(getattr(safe_coords, 'shape', [0])[0]) == 0)\n",
        "\n",
        "            a0 = _rollout_step(g, bot, policy=rollout_policy, model=model, seed_local=game_seed, step_i=steps)\n",
        "            if a0 is None:\n",
        "                break\n",
        "\n",
        "            is_logic_phase = (str(rollout_policy).strip().lower() == 'logic')\n",
        "            mines0 = (int(getattr(g, 'mines_triggered', 0) or 0) == 0)\n",
        "\n",
        "            # I always record forced-guess states (they're rare + highest-signal).\n",
        "            # For the logic phase, I subsample states with record_prob.\n",
        "            if bool(is_logic_phase):\n",
        "                do_record = (recorded < states_cap) and mines0 and (rng.random() < rec_p)\n",
        "            else:\n",
        "                do_record = (recorded < states_cap) and mines0 and bool(forced_guess)\n",
        "            if do_record:\n",
        "                x_int8 = visible_to_int8(visible)\n",
        "\n",
        "                # Candidate set: chosen action + a few alternatives.\n",
        "                # - In the logic phase, I consider all unrevealed cells so the model can learn to\n",
        "                #   prefer better safe moves than the LogicBot's default safe move.\n",
        "                # - In actor rounds, I focus supervision on guess candidates.\n",
        "                if bool(is_logic_phase):\n",
        "                    unrevealed = [(rr, cc) for rr in range(h) for cc in range(w) if visible[rr][cc] == 'E']\n",
        "                else:\n",
        "                    if guess_coords is not None and int(getattr(guess_coords, 'shape', [0])[0]) > 0:\n",
        "                        unrevealed = [(int(rr), int(cc)) for (rr, cc) in guess_coords.tolist()]\n",
        "                    else:\n",
        "                        unrevealed = [(rr, cc) for rr in range(h) for cc in range(w) if visible[rr][cc] == 'E']\n",
        "\n",
        "                candidates = [a0]\n",
        "                pool = [rc for rc in unrevealed if rc != a0]\n",
        "                if len(pool) > 0 and int(actions_per_state) > 1:\n",
        "                    kk = min(len(pool), int(actions_per_state) - 1)\n",
        "                    candidates.extend(rng.sample(pool, k=kk))\n",
        "\n",
        "                for (rr, cc) in candidates:\n",
        "                    if tgt is not None and int(len(xs)) >= int(tgt):\n",
        "                        break\n",
        "                    g2 = _clone_game_fast(g)\n",
        "                    setattr(g2, 'allow_mine_triggers', True)\n",
        "\n",
        "                    prev_m = int(getattr(g2, 'mines_triggered', 0) or 0)\n",
        "                    res0 = g2.player_clicks(int(rr), int(cc), set())\n",
        "                    cur_m = int(getattr(g2, 'mines_triggered', 0) or 0)\n",
        "\n",
        "                    # Mine label is for the CLICK itself.\n",
        "                    ym = 1.0 if (cur_m > prev_m) else 0.0\n",
        "\n",
        "                    # Survival ratio is defined by when the FIRST mine happens (relative to completion).\n",
        "                    # This candidate click is step 1 of the trajectory we label.\n",
        "                    first_mine_at0 = 1 if (prev_m == 0 and cur_m > 0) else None\n",
        "\n",
        "                    steps_total, surv = _rollout_to_end(\n",
        "                        g2,\n",
        "                        policy=rollout_policy,\n",
        "                        model2=model,\n",
        "                        seed_local=int(game_seed) + 999,\n",
        "                        max_extra_steps=int(max_steps) - int(steps) - 1,\n",
        "                        steps0=1,\n",
        "                        first_mine_at0=first_mine_at0,\n",
        "                    )\n",
        "\n",
        "                    ys = float(surv)\n",
        "\n",
        "                    xs.append(x_int8)\n",
        "                    ars.append((int(rr), int(cc)))\n",
        "                    y_survival.append(float(ys))\n",
        "                    y_mine.append(float(ym))\n",
        "                    ep_ids.append(int(ep))\n",
        "\n",
        "                recorded += 1\n",
        "\n",
        "            steps += 1\n",
        "            _ = g.player_clicks(int(a0[0]), int(a0[1]), set())\n",
        "\n",
        "    return {\n",
        "        'x_visible': np.stack(xs).astype(np.int8) if xs else np.zeros((0, h, w), dtype=np.int8),\n",
        "        'action_rc': np.asarray(ars, dtype=np.int16) if ars else np.zeros((0, 2), dtype=np.int16),\n",
        "        'y_survival': np.asarray(y_survival, dtype=np.float32) if y_survival else np.zeros((0,), dtype=np.float32),\n",
        "        'y_mine': np.asarray(y_mine, dtype=np.float32) if y_mine else np.zeros((0,), dtype=np.float32),\n",
        "        'episode_id': np.asarray(ep_ids, dtype=np.int32) if ep_ids else np.zeros((0,), dtype=np.int32),\n",
        "    }\n",
        "\n",
        "\n",
        "def save_dataset_npz(path: Path, data: dict, meta: dict):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    np.savez_compressed(path, **data, meta_json=json.dumps(meta))\n",
        "\n",
        "\n",
        "def load_dataset_npz(path: Path) -> dict:\n",
        "    z = np.load(path, allow_pickle=False)\n",
        "    out = {k: z[k] for k in z.files if k != 'meta_json'}\n",
        "    out['meta'] = json.loads(str(z['meta_json'].tolist()))\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2 — Train a Task 2 value-map model (critic)\n",
        "#\n",
        "# This model outputs:\n",
        "# - value_map[s]   : predicted survival score in [0,1] if we click each cell\n",
        "# - mine_logit_map : predicted P(mine | click)\n",
        "#\n",
        "# I train on sparse (state, action) supervision by gathering the predicted values/logits at the\n",
        "# labeled action coordinates.\n",
        "\n",
        "from dataclasses import asdict\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from models.metrics import regression_metrics\n",
        "from models.task2.value_map_model import BoardValuePredictor, BoardValuePredictorConfig\n",
        "\n",
        "\n",
        "class _Task2SparseDataset(Dataset):\n",
        "    def __init__(self, d: dict, indices: np.ndarray):\n",
        "        self.x = d['x_visible'][indices]\n",
        "        self.a = d['action_rc'][indices]\n",
        "        self.y_surv = d['y_survival'][indices]\n",
        "        self.y_mine = d.get('y_mine', np.zeros_like(self.y_surv))[indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.x.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return {\n",
        "            'x': torch.from_numpy(self.x[idx]).to(torch.int64),\n",
        "            'a': torch.from_numpy(self.a[idx]).to(torch.int64),\n",
        "            'y_survival': torch.tensor(float(self.y_surv[idx]), dtype=torch.float32),\n",
        "            'y_mine': torch.tensor(float(self.y_mine[idx]), dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "\n",
        "def _split_by_episode(episode_id: np.ndarray, *, val_frac: float, seed: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    episode_id = np.asarray(episode_id).astype(np.int64)\n",
        "    uniq = np.unique(episode_id)\n",
        "    rng = np.random.default_rng(int(seed))\n",
        "    rng.shuffle(uniq)\n",
        "\n",
        "    n_val_eps = max(1, int(round(len(uniq) * float(val_frac))))\n",
        "    val_eps = set(int(x) for x in uniq[:n_val_eps])\n",
        "\n",
        "    idx = np.arange(len(episode_id), dtype=np.int64)\n",
        "    val_idx = idx[np.array([int(e) in val_eps for e in episode_id], dtype=bool)]\n",
        "    train_idx = idx[np.array([int(e) not in val_eps for e in episode_id], dtype=bool)]\n",
        "    return train_idx, val_idx\n",
        "\n",
        "\n",
        "def train_value_map_model(\n",
        "    *,\n",
        "    data: dict,\n",
        "    cfg: BoardValuePredictorConfig,\n",
        "    epochs: int = 20,\n",
        "    batch_size: int = 64,\n",
        "    lr: float = 3e-4,\n",
        "    weight_decay: float = 1e-2,\n",
        "    val_frac: float = 0.2,\n",
        "    seed: int = 0,\n",
        "    patience: int = 3,\n",
        "    mine_loss_weight: float = 1.0,\n",
        "):\n",
        "    use_cuda = (device.type == 'cuda')\n",
        "\n",
        "    n = int(data['x_visible'].shape[0])\n",
        "    n_eps = int(len(np.unique(np.asarray(data['episode_id']).astype(np.int64))))\n",
        "    print(\n",
        "        f\"[train] samples={n} episodes={n_eps} epochs={int(epochs)} batch_size={int(batch_size)} lr={float(lr)} \"\n",
        "        f\"mine_w={float(mine_loss_weight)} patience={int(patience)}\"\n",
        "    )\n",
        "\n",
        "    if use_cuda:\n",
        "        try:\n",
        "            torch.backends.cuda.matmul.fp32_precision = 'tf32'\n",
        "            torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    train_idx, val_idx = _split_by_episode(data['episode_id'], val_frac=val_frac, seed=seed)\n",
        "\n",
        "    bs = int(batch_size)\n",
        "    if use_cuda:\n",
        "        try:\n",
        "            if 'A100' in torch.cuda.get_device_name(0):\n",
        "                bs = max(bs, 128)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    num_workers = 4 if use_cuda else 0\n",
        "    dl_common = dict(num_workers=int(num_workers), pin_memory=bool(use_cuda))\n",
        "    if int(num_workers) > 0:\n",
        "        dl_common['persistent_workers'] = True\n",
        "\n",
        "    train_loader = DataLoader(_Task2SparseDataset(data, train_idx), batch_size=int(bs), shuffle=True, **dl_common)\n",
        "    val_loader = DataLoader(_Task2SparseDataset(data, val_idx), batch_size=int(bs), shuffle=False, **dl_common)\n",
        "\n",
        "    print(\n",
        "        f\"[train] split | train_samples={len(train_idx)} val_samples={len(val_idx)} \"\n",
        "        f\"bs={int(bs)} num_workers={int(num_workers)} amp={bool(use_cuda)}\"\n",
        "    )\n",
        "\n",
        "    model = BoardValuePredictor(cfg).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n",
        "\n",
        "    use_amp = bool(use_cuda)\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
        "\n",
        "    best_rmse = float('inf')\n",
        "    best_state = None\n",
        "    best_epoch = 0\n",
        "    bad_epochs = 0\n",
        "\n",
        "    def _gather(map_hw: torch.Tensor, a_rc: torch.Tensor) -> torch.Tensor:\n",
        "        # map_hw: (B,H,W), a_rc: (B,2)\n",
        "        r = a_rc[:, 0].clamp(0, map_hw.shape[1] - 1)\n",
        "        c = a_rc[:, 1].clamp(0, map_hw.shape[2] - 1)\n",
        "        return map_hw[torch.arange(map_hw.shape[0], device=map_hw.device), r, c]\n",
        "\n",
        "    for epoch in range(1, int(epochs) + 1):\n",
        "        model.train()\n",
        "        tr = 0.0\n",
        "        tr_n = 0\n",
        "\n",
        "        for batch_i, batch in enumerate(train_loader):\n",
        "            x = batch['x'].to(device, non_blocking=use_cuda)\n",
        "            a = batch['a'].to(device, non_blocking=use_cuda)\n",
        "            y_surv = batch['y_survival'].to(device, non_blocking=use_cuda)\n",
        "            y_mine = batch['y_mine'].to(device, non_blocking=use_cuda)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast(device_type='cuda', enabled=use_amp):\n",
        "                value_map, mine_logit_map = model(x)\n",
        "                pred_surv = torch.sigmoid(_gather(value_map, a))\n",
        "                pred_mine_logit = _gather(mine_logit_map, a)\n",
        "\n",
        "                # Survival regression (target in [0,1])\n",
        "                loss_steps = F.smooth_l1_loss(pred_surv, y_surv)\n",
        "\n",
        "                # Mine prediction (auxiliary)\n",
        "                # Mine clicks are rare but high-impact, so I upweight them.\n",
        "                w_m = 1.0 + 8.0 * y_mine\n",
        "                loss_mine = (F.binary_cross_entropy_with_logits(pred_mine_logit, y_mine, reduction='none') * w_m).mean()\n",
        "\n",
        "                loss = loss_steps + float(mine_loss_weight) * loss_mine\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "            tr += float(loss.item())\n",
        "            tr_n += 1\n",
        "\n",
        "            if (batch_i % 250) == 0:\n",
        "                print(f\"[train] epoch {epoch}/{epochs} batch {batch_i} | loss {float(loss.item()):.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        ys = []\n",
        "        mine_pred = []\n",
        "        mine_true = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x = batch['x'].to(device, non_blocking=use_cuda)\n",
        "                a = batch['a'].to(device, non_blocking=use_cuda)\n",
        "                y_surv = batch['y_survival'].to(device, non_blocking=use_cuda)\n",
        "                y_mine = batch['y_mine'].to(device, non_blocking=use_cuda)\n",
        "\n",
        "                with torch.amp.autocast(device_type='cuda', enabled=use_amp):\n",
        "                    value_map, mine_logit_map = model(x)\n",
        "                    pred_surv = torch.sigmoid(_gather(value_map, a))\n",
        "                    pred_mine_logit = _gather(mine_logit_map, a)\n",
        "\n",
        "                preds.append(pred_surv.detach().float().cpu())\n",
        "                ys.append(y_surv.detach().float().cpu())\n",
        "                mine_pred.append(torch.sigmoid(pred_mine_logit.detach().float().cpu()))\n",
        "                mine_true.append(y_mine.detach().float().cpu())\n",
        "\n",
        "        val_metrics = regression_metrics(torch.cat(preds, dim=0), torch.cat(ys, dim=0))\n",
        "        mp = torch.cat(mine_pred, dim=0)\n",
        "        mt = torch.cat(mine_true, dim=0)\n",
        "        mine_acc = float(((mp >= 0.5).float() == (mt >= 0.5).float()).float().mean().item()) if int(mt.numel()) else 0.0\n",
        "\n",
        "        cur_rmse = float(val_metrics.get('rmse', 0.0) or 0.0)\n",
        "        print(\n",
        "            f\"epoch {epoch}/{epochs} | \"\n",
        "            f\"train loss {tr/max(1,tr_n):.4f} | \"\n",
        "            f\"val rmse {cur_rmse:.4f} mae {val_metrics['mae']:.4f} corr {val_metrics['corr']:.3f} | \"\n",
        "            f\"mine_acc {mine_acc:.3f}\"\n",
        "        )\n",
        "\n",
        "        if cur_rmse < best_rmse:\n",
        "            best_rmse = cur_rmse\n",
        "            best_epoch = int(epoch)\n",
        "            bad_epochs = 0\n",
        "            best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if int(patience) > 0 and bad_epochs >= int(patience):\n",
        "                print(f\"[train] early stop at epoch {epoch} (best epoch={best_epoch} rmse={best_rmse:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        print(f\"[train] restored best epoch={best_epoch} rmse={best_rmse:.4f}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 — Task 2: Logic model -> Actor -> Critic bootstrapping (easy / medium / hard)\n",
        "#\n",
        "# This follows the instructions provided in the PDF:\n",
        "# - First learn to predict how long the LogicBot survives after choosing a click.\n",
        "# - Then use that model as an Actor: score every move (value map) and take a high-value move.\n",
        "# - Then train a Critic on the Actor's own decisions and bootstrap.\n",
        "\n",
        "from dataclasses import asdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_policy(*, diff: dict, policy: str, model: BoardValuePredictor | None, n_games: int = 80, seed0: int = 0, max_steps: int = 512) -> dict:\n",
        "    rng = random.Random(int(seed0))\n",
        "    stats = []\n",
        "\n",
        "    h = int(diff['height'])\n",
        "    w = int(diff['width'])\n",
        "    m = int(diff['num_mines'])\n",
        "\n",
        "    for _ in range(int(n_games)):\n",
        "        seed = rng.randint(0, 2**31 - 1)\n",
        "        g = MinesweeperGame(height=h, width=w, num_mines=m, seed=int(seed))\n",
        "        setattr(g, 'allow_mine_triggers', True)\n",
        "\n",
        "        first = (rng.randrange(h), rng.randrange(w))\n",
        "        g.player_clicks(int(first[0]), int(first[1]), set())\n",
        "\n",
        "        bot = LogicBot(g, seed=int(seed))\n",
        "\n",
        "        steps = 0\n",
        "        first_mine_at = None\n",
        "        while steps < int(max_steps):\n",
        "            gs = g.get_game_state()\n",
        "            if gs == GameState.PROG:\n",
        "                pass\n",
        "            else:\n",
        "                if not (bool(getattr(g, 'allow_mine_triggers', False)) and gs == GameState.LOST):\n",
        "                    break\n",
        "\n",
        "            kind = str(policy).strip().lower()\n",
        "            if kind == 'logic':\n",
        "                _logic_infer_sets(bot)\n",
        "                a = bot.select_action()\n",
        "                a = None if a is None else (int(a[0]), int(a[1]))\n",
        "            else:\n",
        "                if model is None:\n",
        "                    raise ValueError('policy!=logic needs a model')\n",
        "                a = actor_choose_click(\n",
        "                    model=model,\n",
        "                    game=g,\n",
        "                    bot=bot,\n",
        "                    seed=int(seed) + steps,\n",
        "                    epsilon=float(EPS_EVAL),\n",
        "                    top_k=int(TOPK_EVAL),\n",
        "                )\n",
        "\n",
        "            if a is None:\n",
        "                break\n",
        "\n",
        "            prev_m = int(getattr(g, 'mines_triggered', 0) or 0)\n",
        "            steps += 1\n",
        "            _ = g.player_clicks(int(a[0]), int(a[1]), set())\n",
        "            cur_m = int(getattr(g, 'mines_triggered', 0) or 0)\n",
        "            if first_mine_at is None and prev_m == 0 and cur_m > 0:\n",
        "                first_mine_at = int(steps)\n",
        "\n",
        "        s = g.get_statistics()\n",
        "        s['steps'] = int(steps)\n",
        "        s['first_mine_at'] = int(first_mine_at) if first_mine_at is not None else int(steps)\n",
        "        # Survival ratio = (step of first mine) / (total steps to clear all safe cells)\n",
        "        s['survival'] = float(s['first_mine_at']) / float(max(1, int(steps)))\n",
        "        stats.append(s)\n",
        "\n",
        "    return {\n",
        "        'n': len(stats),\n",
        "        'perfect_win_rate': float(np.mean([bool(s.get('game_won')) and float(s.get('mines_triggered', 0) or 0) == 0.0 for s in stats])),\n",
        "        'avg_survival': float(np.mean([float(s.get('survival', 0) or 0) for s in stats])),\n",
        "        'avg_cells_opened': float(np.mean([float(s.get('cells_opened', 0) or 0) for s in stats])),\n",
        "        'avg_mines_triggered': float(np.mean([float(s.get('mines_triggered', 0) or 0) for s in stats])),\n",
        "    }\n",
        "\n",
        "\n",
        "# I keep old datasets/checkpoints if they already exist (reruns are fast + reproducible).\n",
        "OVERWRITE = False\n",
        "\n",
        "# Easy has very few forced-guess states, so extra bootstrap rounds can end up training on tiny datasets.\n",
        "NUM_ROUNDS = 3  # default\n",
        "NUM_ROUNDS_BY_DIFF = {'easy': 1, 'medium': 3, 'hard': 3}\n",
        "\n",
        "# I scale data and training thresholds by difficulty so bootstrapping has enough signal.\n",
        "GAMES_PER_ROUND_DEFAULT = 40\n",
        "GAMES_PER_ROUND_BY_DIFF = {'easy': 40, 'medium': 80, 'hard': 120}\n",
        "\n",
        "MIN_SAMPLES_TO_TRAIN_DEFAULT = 200\n",
        "MIN_SAMPLES_TO_TRAIN_BY_DIFF = {'easy': 200, 'medium': 400, 'hard': 700}\n",
        "\n",
        "EVAL_GAMES = 80\n",
        "\n",
        "# Dataset size: samples ~ games * states/game * actions/state.\n",
        "# I increase states/game for medium/hard to capture more decision points.\n",
        "STATES_PER_GAME_BY_DIFF = {'easy': 18, 'medium': 28, 'hard': 32}\n",
        "RECORD_PROB_BY_DIFF = {'easy': 0.25, 'medium': 0.30, 'hard': 0.35}\n",
        "ACTIONS_PER_STATE_BY_DIFF = {'easy': 2, 'medium': 2, 'hard': 2}\n",
        "\n",
        "# Actor rounds: I target a minimum number of forced-guess samples so the critic can actually learn\n",
        "# a better guessing policy than random.\n",
        "TARGET_ACTOR_SAMPLES_BY_DIFF = {'easy': 0, 'medium': 1500, 'hard': 2500}\n",
        "\n",
        "# New cache tag so my reruns don’t silently reuse old actor-round datasets/checkpoints.\n",
        "# (I changed data-collection behavior to actually hit target_samples, and I also make easy \"guess-only\".)\n",
        "CACHE_TAG = 'v10_target_samples_easy_guess_only'\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "# I run Task 2 for all difficulties, but I only *optimize* bootstrapping behavior on medium/hard.\n",
        "# Easy and hard are noisy to \"optimize\" via safe-move ranking, so I keep the actor guess-only there.\n",
        "# Medium is the main Task 2 target, so I let the model rank safe moves there.\n",
        "USE_MODEL_ON_SAFE_MOVES_BY_DIFF = {'easy': False, 'medium': True, 'hard': False}\n",
        "\n",
        "for diff_name, diff in DIFFICULTIES.items():\n",
        "    # Set the per-difficulty actor behavior.\n",
        "    USE_MODEL_ON_SAFE_MOVES = bool(USE_MODEL_ON_SAFE_MOVES_BY_DIFF.get(diff_name, True))\n",
        "\n",
        "    # Slightly stronger mine penalty on hard (no extra training cost; just shifts selection).\n",
        "    if str(diff_name) == 'hard':\n",
        "        MINE_PENALTY = 6.0\n",
        "    else:\n",
        "        MINE_PENALTY = 4.0\n",
        "\n",
        "    print('\\n------------------------------')\n",
        "    print('Difficulty:', diff_name, diff)\n",
        "    print('Cache tag:', CACHE_TAG)\n",
        "    print('Actor ranks safe moves with model:', USE_MODEL_ON_SAFE_MOVES)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    games_per_round = int(GAMES_PER_ROUND_BY_DIFF.get(diff_name, int(GAMES_PER_ROUND_DEFAULT)))\n",
        "    min_samples_to_train = int(MIN_SAMPLES_TO_TRAIN_BY_DIFF.get(diff_name, int(MIN_SAMPLES_TO_TRAIN_DEFAULT)))\n",
        "\n",
        "    # ROUND 0: train \"logic survivability\" model\n",
        "    ds0 = DATA_DIR / f'task2_{diff_name}_{CACHE_TAG}_round0_logic.npz'\n",
        "    if ds0.exists() and not bool(OVERWRITE):\n",
        "        data0 = load_dataset_npz(ds0)\n",
        "        meta0 = data0['meta']\n",
        "        print(f\"Loaded: {ds0} samples={data0['x_visible'].shape[0]}\")\n",
        "    else:\n",
        "        data0 = collect_dataset_for_policy(\n",
        "            diff=diff,\n",
        "            rollout_policy='logic',\n",
        "            model=None,\n",
        "            num_games=int(games_per_round),\n",
        "            seed=1000,\n",
        "            states_per_game=int(STATES_PER_GAME_BY_DIFF.get(diff_name, 24)),\n",
        "            record_prob=float(RECORD_PROB_BY_DIFF.get(diff_name, 0.25)),\n",
        "            actions_per_state=int(ACTIONS_PER_STATE_BY_DIFF.get(diff_name, 3)),\n",
        "        )\n",
        "        meta0 = {'task': 'task2_logic_survival', 'difficulty': diff_name, 'difficulty_cfg': diff, 'round': 0}\n",
        "        save_dataset_npz(ds0, data0, meta0)\n",
        "        print(f\"Saved: {ds0} samples={data0['x_visible'].shape[0]}\")\n",
        "\n",
        "    ck0 = CKPT_DIR / f'task2_{diff_name}_{CACHE_TAG}_round0_logic.pt'\n",
        "    if ck0.exists() and not bool(OVERWRITE):\n",
        "        print(f\"[bootstrap] Reusing checkpoint: {ck0}\")\n",
        "        p = torch.load(ck0, map_location=device)\n",
        "        cfg = BoardValuePredictorConfig(**p['model_cfg'])\n",
        "        actor_best = BoardValuePredictor(cfg).to(device)\n",
        "        actor_best.load_state_dict(p['state_dict'])\n",
        "        actor_best.eval()\n",
        "    else:\n",
        "        cfg = BoardValuePredictorConfig(height=int(diff['height']), width=int(diff['width']))\n",
        "        actor_best = train_value_map_model(data=data0, cfg=cfg, epochs=30, batch_size=64, lr=3e-4, weight_decay=1e-2, val_frac=0.2, seed=0, patience=4, mine_loss_weight=1.0)\n",
        "        torch.save({'task': 'task2_logic_survival', 'difficulty': diff_name, 'round': 0, 'model_cfg': asdict(cfg), 'state_dict': actor_best.state_dict()}, ck0)\n",
        "        print(f\"Saved logic-model checkpoint: {ck0}\")\n",
        "\n",
        "    base_stats = eval_policy(diff=diff, policy='logic', model=None, n_games=EVAL_GAMES, seed0=123)\n",
        "    actor0_stats = eval_policy(diff=diff, policy='actor', model=actor_best, n_games=EVAL_GAMES, seed0=5000)\n",
        "    print('LogicBot baseline:', base_stats)\n",
        "    print('Actor (from logic model) stats:', actor0_stats)\n",
        "\n",
        "    def _score(stats: dict) -> tuple[float, float]:\n",
        "        # Primary: perfect wins (aligns with winning the game with 0 mines).\n",
        "        # Secondary: avg_survival (how late the first mine happens relative to completion).\n",
        "        return (float(stats.get('perfect_win_rate', 0.0) or 0.0), float(stats.get('avg_survival', 0.0) or 0.0))\n",
        "\n",
        "    best_score = _score(actor0_stats)\n",
        "    best_round = 0\n",
        "    best_payload = {'task': 'task2_bootstrap', 'difficulty': diff_name, 'difficulty_cfg': diff, 'round': 0, 'model_cfg': asdict(cfg), 'state_dict': {k: v.detach().cpu() for k, v in actor_best.state_dict().items()}}\n",
        "\n",
        "    results.append({'difficulty': diff_name, 'round': 0, 'kind': 'logic_model_actor', **actor0_stats})\n",
        "\n",
        "    # ROUNDS 1+: critic bootstrapping on actor behavior\n",
        "    num_rounds = int(NUM_ROUNDS_BY_DIFF.get(diff_name, int(NUM_ROUNDS)))\n",
        "    for r in range(1, int(num_rounds)):\n",
        "        ds_path = DATA_DIR / f'task2_{diff_name}_{CACHE_TAG}_round{r}_actor.npz'\n",
        "        if ds_path.exists() and not bool(OVERWRITE):\n",
        "            data = load_dataset_npz(ds_path)\n",
        "            meta = data['meta']\n",
        "            print(f\"Loaded: {ds_path} samples={data['x_visible'].shape[0]}\")\n",
        "        else:\n",
        "            tgt = int(TARGET_ACTOR_SAMPLES_BY_DIFF.get(diff_name, 0) or 0) or None\n",
        "            # If I’m targeting forced-guess samples, I allow more games so I can actually hit the target.\n",
        "            # Otherwise I’m guaranteed to be “data-starved” and bootstrapping becomes noisy.\n",
        "            max_games = None if tgt is None else int(games_per_round) * 10\n",
        "\n",
        "            data = collect_dataset_for_policy(\n",
        "                diff=diff,\n",
        "                rollout_policy='actor',\n",
        "                model=actor_best,\n",
        "                num_games=int(games_per_round),\n",
        "                max_games=max_games,\n",
        "                seed=2000 + r,\n",
        "                states_per_game=int(STATES_PER_GAME_BY_DIFF.get(diff_name, 24)),\n",
        "                record_prob=float(RECORD_PROB_BY_DIFF.get(diff_name, 0.25)),\n",
        "                actions_per_state=int(ACTIONS_PER_STATE_BY_DIFF.get(diff_name, 3)),\n",
        "                target_samples=tgt,\n",
        "            )\n",
        "            meta = {'task': 'task2_actor_survival', 'difficulty': diff_name, 'difficulty_cfg': diff, 'round': int(r)}\n",
        "            save_dataset_npz(ds_path, data, meta)\n",
        "            print(f\"Saved: {ds_path} samples={data['x_visible'].shape[0]}\")\n",
        "\n",
        "        if int(data['x_visible'].shape[0]) < int(min_samples_to_train):\n",
        "            print(f\"[bootstrap] Skipping round {r}: only {int(data['x_visible'].shape[0])} samples (<{int(min_samples_to_train)})\")\n",
        "            continue\n",
        "\n",
        "        ckpt_path = CKPT_DIR / f'task2_{diff_name}_{CACHE_TAG}_round{r}_critic.pt'\n",
        "        if ckpt_path.exists() and not bool(OVERWRITE):\n",
        "            print(f\"[bootstrap] Reusing checkpoint: {ckpt_path}\")\n",
        "            p = torch.load(ckpt_path, map_location=device)\n",
        "            ccfg = BoardValuePredictorConfig(**p['model_cfg'])\n",
        "            critic = BoardValuePredictor(ccfg).to(device)\n",
        "            critic.load_state_dict(p['state_dict'])\n",
        "            critic.eval()\n",
        "        else:\n",
        "            ccfg = BoardValuePredictorConfig(height=int(diff['height']), width=int(diff['width']))\n",
        "            critic = train_value_map_model(data=data, cfg=ccfg, epochs=30, batch_size=64, lr=3e-4, weight_decay=1e-2, val_frac=0.2, seed=0, patience=4, mine_loss_weight=1.0)\n",
        "            torch.save({'task': 'task2_actor_critic', 'difficulty': diff_name, 'round': int(r), 'model_cfg': asdict(ccfg), 'state_dict': critic.state_dict()}, ckpt_path)\n",
        "            print(f\"Saved critic checkpoint: {ckpt_path}\")\n",
        "\n",
        "        actor_stats = eval_policy(diff=diff, policy='actor', model=critic, n_games=EVAL_GAMES, seed0=5000 + r)\n",
        "        print(f\"Round {r} actor stats:\", actor_stats)\n",
        "        results.append({'difficulty': diff_name, 'round': int(r), 'kind': 'critic_actor', **actor_stats})\n",
        "\n",
        "        cur_score = _score(actor_stats)\n",
        "        if cur_score > best_score:\n",
        "            best_score = cur_score\n",
        "            best_round = int(r)\n",
        "            actor_best = critic\n",
        "            best_payload = {'task': 'task2_bootstrap', 'difficulty': diff_name, 'difficulty_cfg': diff, 'round': int(r), 'model_cfg': asdict(ccfg), 'state_dict': {k: v.detach().cpu() for k, v in critic.state_dict().items()}}\n",
        "\n",
        "    final_path = CKPT_DIR / f'task2_{diff_name}.pt'\n",
        "    if final_path.exists() and not bool(OVERWRITE):\n",
        "        print(f\"Final already exists: {final_path} (not overwriting; set OVERWRITE=True to regenerate)\")\n",
        "    else:\n",
        "        torch.save(best_payload, final_path)\n",
        "        print(f\"Wrote final: {final_path} (from round {best_round})\")\n",
        "\n",
        "    all_results[diff_name] = results\n",
        "\n",
        "all_results\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
