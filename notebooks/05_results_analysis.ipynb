{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 05 — Results + Data Analysis (Tasks 1–3)\n",
        "\n",
        "I use this notebook to regenerate the figures for my writeups. I save everything to `docs/figures/`.\n",
        "\n",
        "Workflow note: I ran Notebooks 02/03/04 in Colab, then downloaded the repo + artifacts so I could run this notebook locally.\n",
        "\n",
        "Sections:\n",
        "- Task 1: training curves + gameplay vs LogicBot\n",
        "- Task 2: bootstrapping results by round\n",
        "- Task 3: thinking-time plots (reloaded from Notebook 04 exports if present) + heatmaps (generated here from the v1 checkpoints in this repo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# I install plotting deps (I keep it explicit).\n",
        "%pip install -q numpy matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission\n",
            "Figure dir: /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures\n"
          ]
        }
      ],
      "source": [
        "# I locate the repo root and set up `docs/figures/`.\n",
        "# I usually run Notebooks 02/03/04 in Colab, then download the repo/artifacts and run this notebook locally.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _find_repo_root() -> Path:\n",
        "    # Colab default (if I’m running inside Colab)\n",
        "    p = Path('/content/repo')\n",
        "    if p.exists() and (p / 'minesweeper').exists() and (p / 'models').exists():\n",
        "        kids = [k for k in p.iterdir() if k.is_dir()]\n",
        "        if len(kids) == 1 and (kids[0] / 'minesweeper').exists() and (kids[0] / 'models').exists():\n",
        "            return kids[0]\n",
        "        return p\n",
        "\n",
        "    # Local run: walk upward until I see minesweeper/ + models/\n",
        "    for q in [Path.cwd(), *Path.cwd().parents]:\n",
        "        if (q / 'minesweeper').exists() and (q / 'models').exists():\n",
        "            return q\n",
        "\n",
        "    raise FileNotFoundError('I could not find repo root (expected minesweeper/ + models/)')\n",
        "\n",
        "\n",
        "repo_root = _find_repo_root()\n",
        "sys.path.insert(0, str(repo_root))\n",
        "print('Repo root:', repo_root)\n",
        "\n",
        "fig_dir = Path(repo_root) / 'docs' / 'figures'\n",
        "fig_dir.mkdir(parents=True, exist_ok=True)\n",
        "print('Figure dir:', fig_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "easy epochs: 15 final val f1: 0.994\n",
            "medium epochs: 15 final val f1: 0.974\n",
            "hard epochs: 15 final val f1: 0.937\n"
          ]
        }
      ],
      "source": [
        "# Task 1 — I plot training curves\n",
        "# I pasted these metrics straight from my training log so the plots are reproducible.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "metrics = {\n",
        "    'easy': {\n",
        "        'train_loss': [0.2876,0.1761,0.1202,0.0898,0.0695,0.0549,0.0440,0.0363,0.0305,0.0261,0.0226,0.0199,0.0177,0.0160,0.0147],\n",
        "        'val_loss':   [0.2217,0.1457,0.1059,0.0794,0.0652,0.0508,0.0419,0.0354,0.0319,0.0277,0.0252,0.0216,0.0210,0.0202,0.0188],\n",
        "        'train_f1':   [0.901,0.942,0.960,0.969,0.975,0.980,0.984,0.986,0.989,0.990,0.991,0.992,0.993,0.994,0.994],\n",
        "        'val_f1':     [0.926,0.953,0.964,0.973,0.978,0.982,0.985,0.987,0.989,0.991,0.991,0.992,0.993,0.993,0.994],\n",
        "    },\n",
        "    'medium': {\n",
        "        'train_loss': [0.4502,0.3280,0.2398,0.1865,0.1553,0.1349,0.1202,0.1090,0.1002,0.0930,0.0871,0.0821,0.0779,0.0740,0.0707],\n",
        "        'val_loss':   [0.3844,0.2797,0.2091,0.1659,0.1409,0.1229,0.1091,0.1013,0.0923,0.0859,0.0786,0.0756,0.0694,0.0674,0.0632],\n",
        "        'train_f1':   [0.805,0.863,0.900,0.922,0.936,0.944,0.950,0.955,0.959,0.962,0.964,0.966,0.968,0.969,0.971],\n",
        "        'val_f1':     [0.838,0.883,0.914,0.931,0.942,0.950,0.955,0.960,0.963,0.965,0.968,0.970,0.971,0.972,0.974],\n",
        "    },\n",
        "    'hard': {\n",
        "        'train_loss': [0.4370,0.3352,0.2879,0.2578,0.2371,0.2220,0.2104,0.2014,0.1938,0.1875,0.1821,0.1773,0.1732,0.1694,0.1661],\n",
        "        'val_loss':   [0.3728,0.3066,0.2680,0.2416,0.2209,0.2074,0.1950,0.1895,0.1817,0.1735,0.1683,0.1661,0.1603,0.1593,0.1507],\n",
        "        'train_f1':   [0.791,0.849,0.872,0.887,0.897,0.904,0.909,0.913,0.917,0.920,0.922,0.924,0.926,0.928,0.929],\n",
        "        'val_f1':     [0.828,0.863,0.883,0.896,0.904,0.912,0.916,0.919,0.922,0.927,0.929,0.930,0.932,0.933,0.937],\n",
        "    },\n",
        "}\n",
        "\n",
        "for name, d in metrics.items():\n",
        "    n = len(d['train_loss'])\n",
        "    assert all(len(v) == n for v in d.values())\n",
        "    print(name, 'epochs:', n, 'final val f1:', d['val_f1'][-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task1_easy_curves.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task1_medium_curves.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task1_hard_curves.png\n"
          ]
        }
      ],
      "source": [
        "# I plot and save curves (per difficulty)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_curves(name: str, d: dict, out_path: Path):\n",
        "    epochs = np.arange(1, len(d['train_loss']) + 1)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    ax[0].plot(epochs, d['train_loss'], label='train')\n",
        "    ax[0].plot(epochs, d['val_loss'], label='val')\n",
        "    ax[0].set_title(f'{name}: loss vs epoch')\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('masked BCE loss')\n",
        "    ax[0].grid(True, alpha=0.3)\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(epochs, d['train_f1'], label='train')\n",
        "    ax[1].plot(epochs, d['val_f1'], label='val')\n",
        "    ax[1].set_title(f'{name}: mine-F1 vs epoch')\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('mine F1 (masked)')\n",
        "    ax[1].set_ylim(0.0, 1.0)\n",
        "    ax[1].grid(True, alpha=0.3)\n",
        "    ax[1].legend()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "for name, d in metrics.items():\n",
        "    out = fig_dir / f'task1_{name}_curves.png'\n",
        "    plot_curves(name, d, out)\n",
        "    print('wrote', out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 — What I take away\n",
        "\n",
        "Most gains happen early; after that, improvements taper. Train/val stay close, so I don’t see obvious overfitting. Easy saturates; medium/hard are harder (more mines = more ambiguity).\n",
        "\n",
        "## Task 1 — Gameplay vs LogicBot (same boards)\n",
        "\n",
        "I run both bots on the same mine layouts (same first click, then I clone the state).\n",
        "\n",
        "I report:\n",
        "- clear rate\n",
        "- perfect win rate\n",
        "- avg clicks\n",
        "- avg mines triggered\n",
        "\n",
        "I also compute 95% bootstrap confidence intervals.\n",
        "\n",
        "---\n",
        "\n",
        "## Task 2 — Bootstrapping plots\n",
        "\n",
        "I plot:\n",
        "- perfect win rate by round\n",
        "- avg_survival by round\n",
        "- avg_mines_triggered by round\n",
        "- dataset_samples by round\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1 — I evaluate gameplay: LogicBot vs my NN bot\n",
        "import random\n",
        "\n",
        "import torch\n",
        "\n",
        "# I run Notebook 05 locally, so I default to CPU.\n",
        "cpu = torch.device('cpu')\n",
        "\n",
        "from minesweeper.game import GameState, MinesweeperGame\n",
        "from minesweeper.logic_bot import LogicBot\n",
        "from models.task1.model import MinePredictor, MinePredictorConfig\n",
        "from models.task1.policy import select_safest_unrevealed\n",
        "from models.task2.dataset import _clone_game_fast\n",
        "\n",
        "\n",
        "TASK1_CKPT_DIR = Path(repo_root) / 'models' / 'task1' / 'checkpoints'\n",
        "\n",
        "TASK1_DIFFICULTIES = {\n",
        "    'easy': {'height': 22, 'width': 22, 'num_mines': 50},\n",
        "    'medium': {'height': 22, 'width': 22, 'num_mines': 80},\n",
        "    'hard': {'height': 22, 'width': 22, 'num_mines': 100},\n",
        "}\n",
        "\n",
        "\n",
        "def load_task1_model(diff_name: str) -> MinePredictor:\n",
        "    ckpt_path = TASK1_CKPT_DIR / f'task1_{diff_name}.pt'\n",
        "    if not ckpt_path.exists():\n",
        "        raise FileNotFoundError(f'Missing checkpoint: {ckpt_path}')\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=cpu)\n",
        "    cfg = MinePredictorConfig(**(ckpt.get('model_cfg') or {}))\n",
        "    model = MinePredictor(cfg).to(cpu)\n",
        "    model.load_state_dict(ckpt['state_dict'])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def _should_continue(g: MinesweeperGame) -> bool:\n",
        "    gs = g.get_game_state()\n",
        "    if gs == GameState.PROG:\n",
        "        return True\n",
        "    allow = bool(getattr(g, 'allow_mine_triggers', False))\n",
        "    return bool(allow and gs == GameState.LOST)\n",
        "\n",
        "\n",
        "def run_logic_bot(game: MinesweeperGame, *, seed: int, max_clicks: int) -> dict:\n",
        "    bot = LogicBot(game, seed=int(seed))\n",
        "\n",
        "    clicks = 0\n",
        "    first_mine_at = None\n",
        "\n",
        "    while clicks < int(max_clicks) and _should_continue(game):\n",
        "        prev_m = int(getattr(game, 'mines_triggered', 0) or 0)\n",
        "        result, action = bot.play_step()\n",
        "\n",
        "        # LogicBot can emit flag actions; flags aren't clicks and don't change the board.\n",
        "        if isinstance(action, dict) and action.get('type') == 'flag':\n",
        "            continue\n",
        "\n",
        "        if action is not None:\n",
        "            clicks += 1\n",
        "            cur_m = int(getattr(game, 'mines_triggered', 0) or 0)\n",
        "            if first_mine_at is None and prev_m == 0 and cur_m > 0:\n",
        "                first_mine_at = int(clicks)\n",
        "\n",
        "        if result in {'Win', 'Done'}:\n",
        "            break\n",
        "        if result == 'Lost' and not bool(getattr(game, 'allow_mine_triggers', False)):\n",
        "            break\n",
        "\n",
        "    s = game.get_statistics()\n",
        "    if first_mine_at is None:\n",
        "        first_mine_at = int(clicks)\n",
        "\n",
        "    mines = int(s.get('mines_triggered', 0) or 0)\n",
        "    won = bool(s.get('game_won'))\n",
        "\n",
        "    return {\n",
        "        'clicks': int(clicks),\n",
        "        'first_mine_at': int(first_mine_at),\n",
        "        'mines_triggered': int(mines),\n",
        "        'clear': int(won),\n",
        "        'perfect': int(won and mines == 0),\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_nn_bot(game: MinesweeperGame, *, model: MinePredictor, temperature: float, max_clicks: int) -> dict:\n",
        "    clicks = 0\n",
        "    first_mine_at = None\n",
        "\n",
        "    while clicks < int(max_clicks) and _should_continue(game):\n",
        "        a = select_safest_unrevealed(model, game.get_visible_board(), device=cpu, temperature=float(temperature))\n",
        "        if a is None:\n",
        "            break\n",
        "\n",
        "        prev_m = int(getattr(game, 'mines_triggered', 0) or 0)\n",
        "        clicks += 1\n",
        "        _ = game.player_clicks(int(a[0]), int(a[1]), set())\n",
        "        cur_m = int(getattr(game, 'mines_triggered', 0) or 0)\n",
        "        if first_mine_at is None and prev_m == 0 and cur_m > 0:\n",
        "            first_mine_at = int(clicks)\n",
        "\n",
        "    s = game.get_statistics()\n",
        "    if first_mine_at is None:\n",
        "        first_mine_at = int(clicks)\n",
        "\n",
        "    mines = int(s.get('mines_triggered', 0) or 0)\n",
        "    won = bool(s.get('game_won'))\n",
        "\n",
        "    return {\n",
        "        'clicks': int(clicks),\n",
        "        'first_mine_at': int(first_mine_at),\n",
        "        'mines_triggered': int(mines),\n",
        "        'clear': int(won),\n",
        "        'perfect': int(won and mines == 0),\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running easy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running medium\n",
            "running hard\n",
            "\n",
            " easy\n",
            "  logic perfect 0.695 clear 1.0 mines 0.42\n",
            "  nn    perfect 0.4 clear 1.0 mines 1.31\n",
            "\n",
            " medium\n",
            "  logic perfect 0.28 clear 1.0 mines 1.77\n",
            "  nn    perfect 0.02 clear 1.0 mines 5.04\n",
            "\n",
            " hard\n",
            "  logic perfect 0.005 clear 1.0 mines 5.815\n",
            "  nn    perfect 0.01 clear 1.0 mines 6.105\n"
          ]
        }
      ],
      "source": [
        "# Task 1 — I run the comparison on random boards (same board per episode for both bots)\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def bootstrap_ci(x: np.ndarray, *, n_boot: int = 2000, alpha: float = 0.05, seed: int = 0) -> tuple[float, float, float]:\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    if x.size == 0:\n",
        "        return (float('nan'), float('nan'), float('nan'))\n",
        "\n",
        "    rng = np.random.default_rng(int(seed))\n",
        "    n = int(x.size)\n",
        "    means = []\n",
        "    for _ in range(int(n_boot)):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        means.append(float(np.mean(x[idx])))\n",
        "    means = np.asarray(means, dtype=np.float64)\n",
        "\n",
        "    lo = float(np.quantile(means, float(alpha) / 2.0))\n",
        "    hi = float(np.quantile(means, 1.0 - float(alpha) / 2.0))\n",
        "    return (float(np.mean(x)), lo, hi)\n",
        "\n",
        "\n",
        "def eval_task1_vs_logic(*, diff_name: str, n_games: int = 200, seed0: int = 0, temperature: float = 1.0) -> dict:\n",
        "    diff = TASK1_DIFFICULTIES[diff_name]\n",
        "    h = int(diff['height'])\n",
        "    w = int(diff['width'])\n",
        "    m = int(diff['num_mines'])\n",
        "\n",
        "    model = load_task1_model(diff_name)\n",
        "\n",
        "    rng = np.random.default_rng(int(seed0))\n",
        "    seeds = rng.integers(0, 2**31 - 1, size=int(n_games), dtype=np.int64)\n",
        "    first_rs = rng.integers(0, h, size=int(n_games), dtype=np.int64)\n",
        "    first_cs = rng.integers(0, w, size=int(n_games), dtype=np.int64)\n",
        "\n",
        "    out = {\n",
        "        'logic': {k: [] for k in ['clear', 'perfect', 'clicks', 'first_mine_at', 'mines_triggered']},\n",
        "        'nn': {k: [] for k in ['clear', 'perfect', 'clicks', 'first_mine_at', 'mines_triggered']},\n",
        "    }\n",
        "\n",
        "    max_clicks = int(h) * int(w) + 50\n",
        "\n",
        "    for i in range(int(n_games)):\n",
        "        s = int(seeds[i])\n",
        "        r0 = int(first_rs[i])\n",
        "        c0 = int(first_cs[i])\n",
        "\n",
        "        base = MinesweeperGame(height=h, width=w, num_mines=m, seed=int(s))\n",
        "        setattr(base, 'allow_mine_triggers', True)\n",
        "\n",
        "        # I initialize actual_board (mines are placed after the first click).\n",
        "        _ = base.player_clicks(int(r0), int(c0), set())\n",
        "\n",
        "        g_logic = _clone_game_fast(base)\n",
        "        g_nn = _clone_game_fast(base)\n",
        "        setattr(g_logic, 'allow_mine_triggers', True)\n",
        "        setattr(g_nn, 'allow_mine_triggers', True)\n",
        "\n",
        "        a = run_logic_bot(g_logic, seed=int(s) + 1337, max_clicks=max_clicks)\n",
        "        b = run_nn_bot(g_nn, model=model, temperature=float(temperature), max_clicks=max_clicks)\n",
        "\n",
        "        for k in out['logic'].keys():\n",
        "            out['logic'][k].append(a[k])\n",
        "            out['nn'][k].append(b[k])\n",
        "\n",
        "    # I summarize with bootstrap CIs (mean + 95% interval).\n",
        "    summary = {}\n",
        "    for bot_name in ['logic', 'nn']:\n",
        "        summary[bot_name] = {}\n",
        "        for k in out[bot_name].keys():\n",
        "            arr = np.asarray(out[bot_name][k], dtype=np.float64)\n",
        "            mu, lo, hi = bootstrap_ci(arr, seed=int(seed0) + 77)\n",
        "            summary[bot_name][k] = {'mean': mu, 'ci_lo': lo, 'ci_hi': hi}\n",
        "\n",
        "    return {\n",
        "        'diff': diff,\n",
        "        'n': int(n_games),\n",
        "        'raw': out,\n",
        "        'summary': summary,\n",
        "    }\n",
        "\n",
        "\n",
        "N_GAMES = 200  # bump this up for tigheter CIs\n",
        "TEMP = 1.0     # >1 is more exploratory; <1 is more greedy\n",
        "\n",
        "results_task1_play = {}\n",
        "for dn in ['easy', 'medium', 'hard']:\n",
        "    print('running', dn)\n",
        "    results_task1_play[dn] = eval_task1_vs_logic(diff_name=dn, n_games=int(N_GAMES), seed0=42, temperature=float(TEMP))\n",
        "\n",
        "# I print a quick summary\n",
        "for dn in ['easy', 'medium', 'hard']:\n",
        "    s = results_task1_play[dn]['summary']\n",
        "    print('\\n', dn)\n",
        "    print('  logic perfect', s['logic']['perfect']['mean'], 'clear', s['logic']['clear']['mean'], 'mines', s['logic']['mines_triggered']['mean'])\n",
        "    print('  nn    perfect', s['nn']['perfect']['mean'], 'clear', s['nn']['clear']['mean'], 'mines', s['nn']['mines_triggered']['mean'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task1_vs_logic_summary.png\n"
          ]
        }
      ],
      "source": [
        "# I plot LogicBot vs Task 1 NN bot (bootstrap 95% CIs)\n",
        "\n",
        "def _mean_ci(dn: str, bot: str, key: str):\n",
        "    s = results_task1_play[dn]['summary'][bot][key]\n",
        "    mu = float(s['mean'])\n",
        "    lo = float(s['ci_lo'])\n",
        "    hi = float(s['ci_hi'])\n",
        "    return mu, (mu - lo), (hi - mu)\n",
        "\n",
        "\n",
        "def plot_task1_comparison(out_path: Path):\n",
        "    diffs = ['easy', 'medium', 'hard']\n",
        "    bots = ['logic', 'nn']\n",
        "\n",
        "    metrics_to_plot = [\n",
        "        ('perfect', 'perfect win rate (clear with 0 mines)'),\n",
        "        ('clear', 'clear rate (finish even if mines triggered)'),\n",
        "        ('clicks', 'avg clicks'),\n",
        "        ('mines_triggered', 'avg mines triggered'),\n",
        "    ]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    axes = axes.reshape(-1)\n",
        "\n",
        "    width = 0.35\n",
        "    x = np.arange(len(diffs))\n",
        "\n",
        "    for ax_i, (key, title) in enumerate(metrics_to_plot):\n",
        "        ax = axes[ax_i]\n",
        "        for bi, bot in enumerate(bots):\n",
        "            means = []\n",
        "            yerr_lo = []\n",
        "            yerr_hi = []\n",
        "            for dn in diffs:\n",
        "                mu, elo, ehi = _mean_ci(dn, bot, key)\n",
        "                means.append(mu)\n",
        "                yerr_lo.append(elo)\n",
        "                yerr_hi.append(ehi)\n",
        "\n",
        "            ax.bar(x + (bi - 0.5) * width, means, width=width, label=bot)\n",
        "            ax.errorbar(\n",
        "                x + (bi - 0.5) * width,\n",
        "                means,\n",
        "                yerr=[yerr_lo, yerr_hi],\n",
        "                fmt='none',\n",
        "                ecolor='black',\n",
        "                capsize=3,\n",
        "                linewidth=1,\n",
        "            )\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(diffs)\n",
        "        ax.grid(True, axis='y', alpha=0.3)\n",
        "        if key in {'perfect', 'clear'}:\n",
        "            ax.set_ylim(0.0, 1.0)\n",
        "        ax.legend()\n",
        "\n",
        "    fig.suptitle('Task 1: LogicBot vs NN bot (same random boards; 95% bootstrap CI)')\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    fig.savefig(out_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "    print('wrote', out_path)\n",
        "\n",
        "\n",
        "plot_task1_comparison(fig_dir / 'task1_vs_logic_summary.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "easy [(0, 0.95, 0.9952145202425132, 1284)]\n",
            "medium [(0, 0.325, 0.7453056383788143, 3702), (1, 0.4125, 0.8049724640593279, 1500), (2, 0.3875, 0.8278645651605153, 1500)]\n",
            "hard [(0, 0.0625, 0.4453307522010162, 4592), (1, 0.0375, 0.4277399656135703, 2500), (2, 0.05, 0.40203666203711547, 2500)]\n"
          ]
        }
      ],
      "source": [
        "# Task 2 — I load my bootstrapping results dict (copied from my notebook output)\n",
        "# I hardcode it here so the plots are reproducible.\n",
        "\n",
        "results_task2 = {\n",
        "    'easy': [\n",
        "        {\n",
        "            'difficulty': 'easy',\n",
        "            'round': 0,\n",
        "            'kind': 'logic_model_actor',\n",
        "            'n': 80,\n",
        "            'perfect_win_rate': 0.95,\n",
        "            'avg_survival': 0.9952145202425132,\n",
        "            'avg_cells_opened': 434.0,\n",
        "            'avg_mines_triggered': 0.05,\n",
        "            'dataset_samples': 1284,\n",
        "        }\n",
        "    ],\n",
        "    'medium': [\n",
        "        {\n",
        "            'difficulty': 'medium',\n",
        "            'round': 0,\n",
        "            'kind': 'logic_model_actor',\n",
        "            'n': 80,\n",
        "            'perfect_win_rate': 0.325,\n",
        "            'avg_survival': 0.7453056383788143,\n",
        "            'avg_cells_opened': 404.0,\n",
        "            'avg_mines_triggered': 1.25,\n",
        "            'dataset_samples': 3702,\n",
        "        },\n",
        "        {\n",
        "            'difficulty': 'medium',\n",
        "            'round': 1,\n",
        "            'kind': 'critic_actor',\n",
        "            'n': 80,\n",
        "            'perfect_win_rate': 0.4125,\n",
        "            'avg_survival': 0.8049724640593279,\n",
        "            'avg_cells_opened': 404.0,\n",
        "            'avg_mines_triggered': 1.1375,\n",
        "            'dataset_samples': 1500,\n",
        "        },\n",
        "        {\n",
        "            'difficulty': 'medium',\n",
        "            'round': 2,\n",
        "            'kind': 'critic_actor',\n",
        "            'n': 80,\n",
        "            'perfect_win_rate': 0.3875,\n",
        "            'avg_survival': 0.8278645651605153,\n",
        "            'avg_cells_opened': 404.0,\n",
        "            'avg_mines_triggered': 1.0875,\n",
        "            'dataset_samples': 1500,\n",
        "        },\n",
        "    ],\n",
        "    'hard': [\n",
        "        {\n",
        "            'difficulty': 'hard',\n",
        "            'round': 0,\n",
        "            'kind': 'logic_model_actor',\n",
        "            'n': 80,\n",
        "            'perfect_win_rate': 0.0625,\n",
        "            'avg_survival': 0.4453307522010162,\n",
        "            'avg_cells_opened': 384.0,\n",
        "            'avg_mines_triggered': 3.35,\n",
        "            'dataset_samples': 4592,\n",
        "        },\n",
        "        {\n",
        "            'difficulty': 'hard',\n",
        "            'round': 1,\n",
        "            'kind': 'critic_actor',\n",
        "            'n': 80,\n",
        "            'perfect_win_rate': 0.0375,\n",
        "            'avg_survival': 0.4277399656135703,\n",
        "            'avg_cells_opened': 384.0,\n",
        "            'avg_mines_triggered': 4.4625,\n",
        "            'dataset_samples': 2500,\n",
        "        },\n",
        "        {\n",
        "            'difficulty': 'hard',\n",
        "            'round': 2,\n",
        "            'kind': 'critic_actor',\n",
        "            'n': 80,\n",
        "            'perfect_win_rate': 0.05,\n",
        "            'avg_survival': 0.40203666203711547,\n",
        "            'avg_cells_opened': 384.0,\n",
        "            'avg_mines_triggered': 4.925,\n",
        "            'dataset_samples': 2500,\n",
        "        },\n",
        "    ],\n",
        "}\n",
        "\n",
        "for d, rows in results_task2.items():\n",
        "    rows = sorted(rows, key=lambda r: int(r['round']))\n",
        "    print(d, [(r['round'], r['perfect_win_rate'], r['avg_survival'], r['dataset_samples']) for r in rows])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task2_perfect_win_rate.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task2_avg_survival.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task2_avg_mines_triggered.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task2_dataset_sizes.png\n"
          ]
        }
      ],
      "source": [
        "# Task 2 — I plot the bootstrapping results (saved into docs/figures)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_task2_metric(metric: str, title: str, out_name: str, *, ylim=None):\n",
        "    diffs = ['easy', 'medium', 'hard']\n",
        "    rounds = sorted({int(r['round']) for d in diffs for r in results_task2[d]})\n",
        "\n",
        "    x = np.arange(len(rounds))\n",
        "    width = 0.25\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 4))\n",
        "\n",
        "    for i, d in enumerate(diffs):\n",
        "        rmap = {int(r['round']): r for r in results_task2[d]}\n",
        "        vals = [float(rmap[rr][metric]) if rr in rmap else np.nan for rr in rounds]\n",
        "        ax.bar(x + (i - 1) * width, vals, width=width, label=d)\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([f'round {r}' for r in rounds])\n",
        "    ax.grid(True, axis='y', alpha=0.3)\n",
        "    if ylim is not None:\n",
        "        ax.set_ylim(*ylim)\n",
        "    ax.legend()\n",
        "\n",
        "    out_path = fig_dir / out_name\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "    print('wrote', out_path)\n",
        "\n",
        "\n",
        "plot_task2_metric('perfect_win_rate', 'Task 2: perfect win rate by round', 'task2_perfect_win_rate.png', ylim=(0.0, 1.0))\n",
        "plot_task2_metric('avg_survival', 'Task 2: avg survival by round', 'task2_avg_survival.png', ylim=(0.0, 1.0))\n",
        "plot_task2_metric('avg_mines_triggered', 'Task 2: avg mines triggered by round', 'task2_avg_mines_triggered.png')\n",
        "plot_task2_metric('dataset_samples', 'Task 2: dataset sizes by round', 'task2_dataset_sizes.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 3 — Thinking longer\n",
        "\n",
        "I’m checking three things:\n",
        "- loss vs thinking steps\n",
        "- win rate vs thinking steps\n",
        "- heatmap evolution on the same board\n",
        "\n",
        "v2 training snapshot (from my Colab logs I pasted earlier):\n",
        "- easy: val loss 0.0176, val F1 0.995 (25 epochs)\n",
        "- medium: val loss 0.0468, val F1 0.982 (35 epochs)\n",
        "- hard: val loss 0.1233, val F1 0.949 (40 epochs)\n",
        "\n",
        "Important note: I couldn’t find / recover my v2 checkpoints locally (Colab disconnected when I fell asleep overnight), so in this repo I only have the **v1** Task 3 checkpoints. The heatmaps below are therefore generated from **v1** only.\n",
        "\n",
        "Notebook 04 also exports Task 3 eval outputs into `docs/figures/` (JSON + PNG). Below I reload and replot those if they exist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3 — Training log transcript (from my Colab run)\n",
        "\n",
        "I pasted this directly from my Colab output. I’m keeping it here so the loss-curve plots below are reproducible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task3_loss_curves_from_logs.png\n",
            "easy epochs: 25 final train loss: 0.0091 final val loss: 0.0176\n",
            "medium epochs: 35 final train loss: 0.0508 final val loss: 0.0468\n",
            "hard epochs: 40 final train loss: 0.1315 final val loss: 0.1233\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task3_easy_loss_curve.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task3_medium_loss_curve.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task3_hard_loss_curve.png\n"
          ]
        }
      ],
      "source": [
        "# Task 3 — Loss curves (parsed from my training log transcript)\n",
        "# I use the per-epoch lines I pasted above to plot train/val loss vs epoch, just like Task 1.\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "TASK3_TRAIN_LOG = r\"\"\"\n",
        "Training Task 3 model for easy...\n",
        "[easy] train_task3: samples=73058 steps_train=4 epochs=25 batch_size=64 seed=0\n",
        "[easy] epoch 1/25 | train loss 0.3141 acc 0.930 prec 0.955 rec 0.834 f1 0.890 | val loss 0.2545 acc 0.944 prec 0.963 rec 0.869 f1 0.914\n",
        "[easy] epoch 2/25 | train loss 0.2009 acc 0.957 prec 0.978 rec 0.894 f1 0.934 | val loss 0.1741 acc 0.964 prec 0.982 rec 0.910 f1 0.945\n",
        "[easy] epoch 3/25 | train loss 0.1449 acc 0.969 prec 0.982 rec 0.926 f1 0.953 | val loss 0.1366 acc 0.972 prec 0.986 rec 0.929 f1 0.957\n",
        "[easy] epoch 4/25 | train loss 0.1154 acc 0.975 prec 0.985 rec 0.941 f1 0.962 | val loss 0.1153 acc 0.976 prec 0.985 rec 0.942 f1 0.963\n",
        "[easy] epoch 5/25 | train loss 0.0962 acc 0.979 prec 0.986 rec 0.952 f1 0.968 | val loss 0.0991 acc 0.979 prec 0.984 rec 0.952 f1 0.968\n",
        "[easy] epoch 6/25 | train loss 0.0816 acc 0.982 prec 0.986 rec 0.960 f1 0.973 | val loss 0.0851 acc 0.981 prec 0.987 rec 0.957 f1 0.972\n",
        "[easy] epoch 7/25 | train loss 0.0690 acc 0.984 prec 0.986 rec 0.967 f1 0.976 | val loss 0.0718 acc 0.984 prec 0.989 rec 0.964 f1 0.976\n",
        "[easy] epoch 8/25 | train loss 0.0578 acc 0.986 prec 0.987 rec 0.973 f1 0.980 | val loss 0.0619 acc 0.987 prec 0.991 rec 0.969 f1 0.980\n",
        "[easy] epoch 9/25 | train loss 0.0483 acc 0.988 prec 0.987 rec 0.978 f1 0.983 | val loss 0.0507 acc 0.989 prec 0.990 rec 0.977 f1 0.983\n",
        "[easy] epoch 10/25 | train loss 0.0408 acc 0.990 prec 0.988 rec 0.983 f1 0.985 | val loss 0.0434 acc 0.990 prec 0.990 rec 0.981 f1 0.986\n",
        "[easy] epoch 11/25 | train loss 0.0345 acc 0.991 prec 0.989 rec 0.986 f1 0.987 | val loss 0.0368 acc 0.992 prec 0.989 rec 0.986 f1 0.988\n",
        "[easy] epoch 12/25 | train loss 0.0292 acc 0.993 prec 0.990 rec 0.989 f1 0.989 | val loss 0.0353 acc 0.992 prec 0.990 rec 0.987 f1 0.988\n",
        "[easy] epoch 13/25 | train loss 0.0255 acc 0.994 prec 0.991 rec 0.991 f1 0.991 | val loss 0.0309 acc 0.993 prec 0.991 rec 0.989 f1 0.990\n",
        "[easy] epoch 14/25 | train loss 0.0223 acc 0.994 prec 0.992 rec 0.992 f1 0.992 | val loss 0.0267 acc 0.994 prec 0.992 rec 0.991 f1 0.991\n",
        "[easy] epoch 15/25 | train loss 0.0197 acc 0.995 prec 0.992 rec 0.993 f1 0.993 | val loss 0.0267 acc 0.994 prec 0.992 rec 0.991 f1 0.991\n",
        "[easy] epoch 16/25 | train loss 0.0177 acc 0.996 prec 0.993 rec 0.994 f1 0.993 | val loss 0.0247 acc 0.994 prec 0.991 rec 0.992 f1 0.992\n",
        "[easy] epoch 17/25 | train loss 0.0162 acc 0.996 prec 0.994 rec 0.995 f1 0.994 | val loss 0.0229 acc 0.995 prec 0.994 rec 0.993 f1 0.993\n",
        "[easy] epoch 18/25 | train loss 0.0147 acc 0.996 prec 0.994 rec 0.995 f1 0.995 | val loss 0.0235 acc 0.996 prec 0.995 rec 0.991 f1 0.993\n",
        "[easy] epoch 19/25 | train loss 0.0135 acc 0.997 prec 0.994 rec 0.996 f1 0.995 | val loss 0.0210 acc 0.996 prec 0.993 rec 0.994 f1 0.993\n",
        "[easy] epoch 20/25 | train loss 0.0126 acc 0.997 prec 0.995 rec 0.996 f1 0.995 | val loss 0.0197 acc 0.996 prec 0.994 rec 0.994 f1 0.994\n",
        "[easy] epoch 21/25 | train loss 0.0118 acc 0.997 prec 0.995 rec 0.996 f1 0.996 | val loss 0.0192 acc 0.996 prec 0.994 rec 0.994 f1 0.994\n",
        "[easy] epoch 22/25 | train loss 0.0109 acc 0.997 prec 0.995 rec 0.997 f1 0.996 | val loss 0.0196 acc 0.996 prec 0.995 rec 0.994 f1 0.995\n",
        "[easy] epoch 23/25 | train loss 0.0105 acc 0.997 prec 0.996 rec 0.997 f1 0.996 | val loss 0.0185 acc 0.997 prec 0.995 rec 0.995 f1 0.995\n",
        "[easy] epoch 24/25 | train loss 0.0098 acc 0.998 prec 0.996 rec 0.997 f1 0.996 | val loss 0.0176 acc 0.996 prec 0.994 rec 0.996 f1 0.995\n",
        "[easy] epoch 25/25 | train loss 0.0091 acc 0.998 prec 0.996 rec 0.997 f1 0.997 | val loss 0.0176 acc 0.997 prec 0.995 rec 0.996 f1 0.995\n",
        "\n",
        "Training Task 3 model for medium...\n",
        "[medium] train_task3: samples=316187 steps_train=4 epochs=35 batch_size=64 seed=0\n",
        "[medium] epoch 1/35 | train loss 0.4451 acc 0.883 prec 0.943 rec 0.706 f1 0.807 | val loss 0.3785 acc 0.899 prec 0.934 rec 0.762 f1 0.839\n",
        "[medium] epoch 2/35 | train loss 0.3379 acc 0.910 prec 0.943 rec 0.790 f1 0.860 | val loss 0.3067 acc 0.918 prec 0.941 rec 0.814 f1 0.873\n",
        "[medium] epoch 3/35 | train loss 0.2819 acc 0.924 prec 0.941 rec 0.833 f1 0.884 | val loss 0.2582 acc 0.929 prec 0.936 rec 0.854 f1 0.893\n",
        "[medium] epoch 4/35 | train loss 0.2339 acc 0.935 prec 0.936 rec 0.873 f1 0.903 | val loss 0.2082 acc 0.943 prec 0.948 rec 0.886 f1 0.916\n",
        "[medium] epoch 5/35 | train loss 0.1932 acc 0.946 prec 0.938 rec 0.904 f1 0.921 | val loss 0.1714 acc 0.951 prec 0.937 rec 0.919 f1 0.928\n",
        "[medium] epoch 6/35 | train loss 0.1639 acc 0.954 prec 0.944 rec 0.924 f1 0.934 | val loss 0.1487 acc 0.958 prec 0.946 rec 0.931 f1 0.939\n",
        "[medium] epoch 7/35 | train loss 0.1442 acc 0.960 prec 0.949 rec 0.935 f1 0.942 | val loss 0.1326 acc 0.961 prec 0.944 rec 0.944 f1 0.944\n",
        "[medium] epoch 8/35 | train loss 0.1300 acc 0.964 prec 0.953 rec 0.943 f1 0.948 | val loss 0.1194 acc 0.966 prec 0.952 rec 0.949 f1 0.950\n",
        "[medium] epoch 9/35 | train loss 0.1192 acc 0.967 prec 0.956 rec 0.949 f1 0.953 | val loss 0.1090 acc 0.970 prec 0.962 rec 0.950 f1 0.956\n",
        "[medium] epoch 10/35 | train loss 0.1107 acc 0.970 prec 0.959 rec 0.954 f1 0.956 | val loss 0.1022 acc 0.971 prec 0.961 rec 0.956 f1 0.958\n",
        "[medium] epoch 11/35 | train loss 0.1037 acc 0.972 prec 0.961 rec 0.958 f1 0.959 | val loss 0.0962 acc 0.972 prec 0.958 rec 0.962 f1 0.960\n",
        "[medium] epoch 12/35 | train loss 0.0979 acc 0.974 prec 0.963 rec 0.961 f1 0.962 | val loss 0.0903 acc 0.975 prec 0.966 rec 0.961 f1 0.964\n",
        "[medium] epoch 13/35 | train loss 0.0928 acc 0.975 prec 0.965 rec 0.963 f1 0.964 | val loss 0.0859 acc 0.976 prec 0.969 rec 0.962 f1 0.966\n",
        "[medium] epoch 14/35 | train loss 0.0884 acc 0.976 prec 0.966 rec 0.965 f1 0.966 | val loss 0.0814 acc 0.977 prec 0.970 rec 0.965 f1 0.967\n",
        "[medium] epoch 15/35 | train loss 0.0846 acc 0.977 prec 0.968 rec 0.967 f1 0.967 | val loss 0.0786 acc 0.978 prec 0.971 rec 0.966 f1 0.969\n",
        "[medium] epoch 16/35 | train loss 0.0812 acc 0.978 prec 0.969 rec 0.969 f1 0.969 | val loss 0.0766 acc 0.978 prec 0.967 rec 0.970 f1 0.968\n",
        "[medium] epoch 17/35 | train loss 0.0783 acc 0.979 prec 0.970 rec 0.970 f1 0.970 | val loss 0.0719 acc 0.980 prec 0.970 rec 0.971 f1 0.971\n",
        "[medium] epoch 18/35 | train loss 0.0756 acc 0.980 prec 0.971 rec 0.971 f1 0.971 | val loss 0.0698 acc 0.981 prec 0.974 rec 0.970 f1 0.972\n",
        "[medium] epoch 19/35 | train loss 0.0731 acc 0.981 prec 0.972 rec 0.972 f1 0.972 | val loss 0.0676 acc 0.982 prec 0.976 rec 0.971 f1 0.973\n",
        "[medium] epoch 20/35 | train loss 0.0709 acc 0.981 prec 0.973 rec 0.974 f1 0.973 | val loss 0.0664 acc 0.982 prec 0.977 rec 0.971 f1 0.974\n",
        "[medium] epoch 21/35 | train loss 0.0688 acc 0.982 prec 0.973 rec 0.975 f1 0.974 | val loss 0.0660 acc 0.982 prec 0.978 rec 0.971 f1 0.974\n",
        "[medium] epoch 22/35 | train loss 0.0669 acc 0.982 prec 0.974 rec 0.975 f1 0.975 | val loss 0.0624 acc 0.982 prec 0.975 rec 0.975 f1 0.975\n",
        "[medium] epoch 23/35 | train loss 0.0651 acc 0.983 prec 0.975 rec 0.976 f1 0.975 | val loss 0.0609 acc 0.983 prec 0.974 rec 0.976 f1 0.975\n",
        "[medium] epoch 24/35 | train loss 0.0635 acc 0.983 prec 0.975 rec 0.977 f1 0.976 | val loss 0.0590 acc 0.984 prec 0.977 rec 0.976 f1 0.977\n",
        "[medium] epoch 25/35 | train loss 0.0619 acc 0.984 prec 0.976 rec 0.978 f1 0.977 | val loss 0.0573 acc 0.984 prec 0.977 rec 0.977 f1 0.977\n",
        "[medium] epoch 26/35 | train loss 0.0605 acc 0.984 prec 0.976 rec 0.978 f1 0.977 | val loss 0.0556 acc 0.984 prec 0.976 rec 0.979 f1 0.978\n",
        "[medium] epoch 27/35 | train loss 0.0592 acc 0.985 prec 0.977 rec 0.979 f1 0.978 | val loss 0.0555 acc 0.984 prec 0.975 rec 0.979 f1 0.977\n",
        "[medium] epoch 28/35 | train loss 0.0578 acc 0.985 prec 0.977 rec 0.980 f1 0.978 | val loss 0.0535 acc 0.985 prec 0.978 rec 0.979 f1 0.979\n",
        "[medium] epoch 29/35 | train loss 0.0567 acc 0.985 prec 0.978 rec 0.980 f1 0.979 | val loss 0.0522 acc 0.986 prec 0.980 rec 0.979 f1 0.979\n",
        "[medium] epoch 30/35 | train loss 0.0555 acc 0.986 prec 0.978 rec 0.980 f1 0.979 | val loss 0.0511 acc 0.986 prec 0.979 rec 0.980 f1 0.979\n",
        "[medium] epoch 31/35 | train loss 0.0545 acc 0.986 prec 0.979 rec 0.981 f1 0.980 | val loss 0.0510 acc 0.986 prec 0.981 rec 0.979 f1 0.980\n",
        "[medium] epoch 32/35 | train loss 0.0535 acc 0.986 prec 0.979 rec 0.981 f1 0.980 | val loss 0.0484 acc 0.986 prec 0.977 rec 0.983 f1 0.980\n",
        "[medium] epoch 33/35 | train loss 0.0525 acc 0.987 prec 0.979 rec 0.982 f1 0.981 | val loss 0.0489 acc 0.986 prec 0.977 rec 0.983 f1 0.980\n",
        "[medium] epoch 34/35 | train loss 0.0516 acc 0.987 prec 0.980 rec 0.982 f1 0.981 | val loss 0.0473 acc 0.987 prec 0.980 rec 0.982 f1 0.981\n",
        "[medium] epoch 35/35 | train loss 0.0508 acc 0.987 prec 0.980 rec 0.983 f1 0.981 | val loss 0.0468 acc 0.987 prec 0.982 rec 0.981 f1 0.982\n",
        "\n",
        "Training Task 3 model for hard...\n",
        "[hard] train_task3: samples=621444 steps_train=4 epochs=40 batch_size=64 seed=0\n",
        "[hard] epoch 1/40 | train loss 0.4389 acc 0.859 prec 0.937 rec 0.683 f1 0.790 | val loss 0.3796 acc 0.880 prec 0.937 rec 0.738 f1 0.826\n",
        "[hard] epoch 2/40 | train loss 0.3441 acc 0.890 prec 0.937 rec 0.768 f1 0.844 | val loss 0.3224 acc 0.895 prec 0.924 rec 0.795 f1 0.855\n",
        "[hard] epoch 3/40 | train loss 0.2990 acc 0.904 prec 0.933 rec 0.811 f1 0.868 | val loss 0.2849 acc 0.909 prec 0.935 rec 0.823 f1 0.875\n",
        "[hard] epoch 4/40 | train loss 0.2670 acc 0.915 prec 0.931 rec 0.841 f1 0.884 | val loss 0.2521 acc 0.920 prec 0.939 rec 0.848 f1 0.891\n",
        "[hard] epoch 5/40 | train loss 0.2438 acc 0.922 prec 0.932 rec 0.862 f1 0.896 | val loss 0.2355 acc 0.921 prec 0.913 rec 0.880 f1 0.896\n",
        "[hard] epoch 6/40 | train loss 0.2268 acc 0.928 prec 0.934 rec 0.876 f1 0.904 | val loss 0.2140 acc 0.931 prec 0.937 rec 0.881 f1 0.908\n",
        "[hard] epoch 7/40 | train loss 0.2141 acc 0.932 prec 0.936 rec 0.885 f1 0.910 | val loss 0.2044 acc 0.934 prec 0.936 rec 0.890 f1 0.913\n",
        "[hard] epoch 8/40 | train loss 0.2043 acc 0.936 prec 0.938 rec 0.892 f1 0.915 | val loss 0.1933 acc 0.938 prec 0.943 rec 0.894 f1 0.918\n",
        "[hard] epoch 9/40 | train loss 0.1963 acc 0.938 prec 0.940 rec 0.898 f1 0.918 | val loss 0.1909 acc 0.940 prec 0.944 rec 0.897 f1 0.920\n",
        "[hard] epoch 10/40 | train loss 0.1899 acc 0.940 prec 0.941 rec 0.902 f1 0.921 | val loss 0.1847 acc 0.940 prec 0.937 rec 0.905 f1 0.921\n",
        "[hard] epoch 11/40 | train loss 0.1844 acc 0.942 prec 0.943 rec 0.906 f1 0.924 | val loss 0.1799 acc 0.942 prec 0.940 rec 0.908 f1 0.924\n",
        "[hard] epoch 12/40 | train loss 0.1795 acc 0.944 prec 0.944 rec 0.909 f1 0.926 | val loss 0.1732 acc 0.945 prec 0.948 rec 0.908 f1 0.927\n",
        "[hard] epoch 13/40 | train loss 0.1753 acc 0.945 prec 0.945 rec 0.912 f1 0.928 | val loss 0.1778 acc 0.944 prec 0.948 rec 0.905 f1 0.926\n",
        "[hard] epoch 14/40 | train loss 0.1716 acc 0.947 prec 0.946 rec 0.914 f1 0.930 | val loss 0.1658 acc 0.947 prec 0.948 rec 0.914 f1 0.930\n",
        "[hard] epoch 15/40 | train loss 0.1683 acc 0.948 prec 0.947 rec 0.917 f1 0.931 | val loss 0.1599 acc 0.949 prec 0.948 rec 0.918 f1 0.933\n",
        "[hard] epoch 16/40 | train loss 0.1653 acc 0.949 prec 0.947 rec 0.919 f1 0.933 | val loss 0.1561 acc 0.950 prec 0.951 rec 0.919 f1 0.935\n",
        "[hard] epoch 17/40 | train loss 0.1625 acc 0.950 prec 0.948 rec 0.920 f1 0.934 | val loss 0.1588 acc 0.949 prec 0.947 rec 0.920 f1 0.933\n",
        "[hard] epoch 18/40 | train loss 0.1602 acc 0.950 prec 0.949 rec 0.922 f1 0.935 | val loss 0.1517 acc 0.952 prec 0.954 rec 0.921 f1 0.937\n",
        "[hard] epoch 19/40 | train loss 0.1577 acc 0.951 prec 0.949 rec 0.923 f1 0.936 | val loss 0.1513 acc 0.952 prec 0.952 rec 0.922 f1 0.937\n",
        "[hard] epoch 20/40 | train loss 0.1558 acc 0.952 prec 0.950 rec 0.924 f1 0.937 | val loss 0.1480 acc 0.953 prec 0.955 rec 0.923 f1 0.938\n",
        "[hard] epoch 21/40 | train loss 0.1536 acc 0.953 prec 0.951 rec 0.926 f1 0.938 | val loss 0.1523 acc 0.953 prec 0.965 rec 0.913 f1 0.938\n",
        "[hard] epoch 22/40 | train loss 0.1519 acc 0.953 prec 0.951 rec 0.927 f1 0.939 | val loss 0.1424 acc 0.955 prec 0.959 rec 0.924 f1 0.941\n",
        "[hard] epoch 23/40 | train loss 0.1500 acc 0.954 prec 0.952 rec 0.928 f1 0.940 | val loss 0.1410 acc 0.955 prec 0.955 rec 0.928 f1 0.941\n",
        "[hard] epoch 24/40 | train loss 0.1484 acc 0.954 prec 0.952 rec 0.929 f1 0.940 | val loss 0.1412 acc 0.955 prec 0.957 rec 0.927 f1 0.941\n",
        "[hard] epoch 25/40 | train loss 0.1469 acc 0.955 prec 0.952 rec 0.930 f1 0.941 | val loss 0.1410 acc 0.955 prec 0.954 rec 0.929 f1 0.941\n",
        "[hard] epoch 26/40 | train loss 0.1456 acc 0.955 prec 0.953 rec 0.931 f1 0.942 | val loss 0.1368 acc 0.956 prec 0.954 rec 0.932 f1 0.943\n",
        "[hard] epoch 27/40 | train loss 0.1441 acc 0.956 prec 0.953 rec 0.932 f1 0.942 | val loss 0.1385 acc 0.956 prec 0.957 rec 0.928 f1 0.943\n",
        "[hard] epoch 28/40 | train loss 0.1429 acc 0.956 prec 0.954 rec 0.932 f1 0.943 | val loss 0.1350 acc 0.957 prec 0.956 rec 0.932 f1 0.944\n",
        "[hard] epoch 29/40 | train loss 0.1417 acc 0.957 prec 0.954 rec 0.933 f1 0.943 | val loss 0.1351 acc 0.957 prec 0.955 rec 0.933 f1 0.944\n",
        "[hard] epoch 30/40 | train loss 0.1405 acc 0.957 prec 0.954 rec 0.934 f1 0.944 | val loss 0.1337 acc 0.958 prec 0.961 rec 0.930 f1 0.945\n",
        "[hard] epoch 31/40 | train loss 0.1394 acc 0.957 prec 0.955 rec 0.934 f1 0.944 | val loss 0.1320 acc 0.958 prec 0.958 rec 0.933 f1 0.945\n",
        "[hard] epoch 32/40 | train loss 0.1384 acc 0.958 prec 0.955 rec 0.935 f1 0.945 | val loss 0.1294 acc 0.959 prec 0.958 rec 0.935 f1 0.946\n",
        "[hard] epoch 33/40 | train loss 0.1373 acc 0.958 prec 0.955 rec 0.936 f1 0.945 | val loss 0.1299 acc 0.959 prec 0.958 rec 0.934 f1 0.946\n",
        "[hard] epoch 34/40 | train loss 0.1365 acc 0.958 prec 0.955 rec 0.936 f1 0.946 | val loss 0.1277 acc 0.959 prec 0.957 rec 0.937 f1 0.947\n",
        "[hard] epoch 35/40 | train loss 0.1355 acc 0.959 prec 0.956 rec 0.937 f1 0.946 | val loss 0.1277 acc 0.960 prec 0.960 rec 0.935 f1 0.947\n",
        "[hard] epoch 36/40 | train loss 0.1346 acc 0.959 prec 0.956 rec 0.937 f1 0.946 | val loss 0.1292 acc 0.959 prec 0.958 rec 0.936 f1 0.946\n",
        "[hard] epoch 37/40 | train loss 0.1338 acc 0.959 prec 0.956 rec 0.938 f1 0.947 | val loss 0.1270 acc 0.959 prec 0.952 rec 0.941 f1 0.946\n",
        "[hard] epoch 38/40 | train loss 0.1331 acc 0.959 prec 0.956 rec 0.938 f1 0.947 | val loss 0.1284 acc 0.959 prec 0.955 rec 0.938 f1 0.946\n",
        "[hard] epoch 39/40 | train loss 0.1322 acc 0.960 prec 0.957 rec 0.939 f1 0.948 | val loss 0.1229 acc 0.962 prec 0.964 rec 0.936 f1 0.950\n",
        "[hard] epoch 40/40 | train loss 0.1315 acc 0.960 prec 0.957 rec 0.939 f1 0.948 | val loss 0.1233 acc 0.961 prec 0.961 rec 0.938 f1 0.949\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def parse_task3_epoch_losses(log_text: str):\n",
        "    # I parse per-epoch train/val loss for each difficulty.\n",
        "    pat = re.compile(\n",
        "        r\"^(?:\\[(?P<diff>[^\\]]+)\\]\\s*)?epoch\\s+(?P<epoch>\\d+)\\/(?P<epochs>\\d+)\\s*\\|\\s*\"\n",
        "        r\"train\\s+loss\\s+(?P<tr>\\d+(?:\\.\\d+)?)\\b.*?\\|\\s*\"\n",
        "        r\"val\\s+loss\\s+(?P<va>\\d+(?:\\.\\d+)?)\\b\",\n",
        "        flags=re.IGNORECASE | re.MULTILINE,\n",
        "    )\n",
        "\n",
        "    out = {}\n",
        "    for m in pat.finditer(log_text or ''):\n",
        "        diff = (m.group('diff') or '').strip().lower()\n",
        "        if diff in {'e', 'ez'}:\n",
        "            diff = 'easy'\n",
        "        if diff in {'m', 'med'}:\n",
        "            diff = 'medium'\n",
        "        if diff in {'h'}:\n",
        "            diff = 'hard'\n",
        "        if diff not in {'easy', 'medium', 'hard'}:\n",
        "            diff = diff or 'unknown'\n",
        "\n",
        "        d = out.setdefault(diff, {'epoch': [], 'train_loss': [], 'val_loss': []})\n",
        "        d['epoch'].append(int(m.group('epoch')))\n",
        "        d['train_loss'].append(float(m.group('tr')))\n",
        "        d['val_loss'].append(float(m.group('va')))\n",
        "\n",
        "    for diff, d in out.items():\n",
        "        order = np.argsort(np.asarray(d['epoch'], dtype=np.int64))\n",
        "        for k in ['epoch', 'train_loss', 'val_loss']:\n",
        "            arr = np.asarray(d[k])\n",
        "            d[k] = [arr[i].item() for i in order]\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def plot_task3_loss_curves_from_logs(log_text: str, *, out_path: Path) -> dict:\n",
        "    parsed = parse_task3_epoch_losses(log_text)\n",
        "    if not parsed:\n",
        "        raise RuntimeError('No per-epoch loss lines found in TASK3_TRAIN_LOG.')\n",
        "\n",
        "    diffs = [d for d in ['easy', 'medium', 'hard'] if d in parsed]\n",
        "    if not diffs:\n",
        "        diffs = list(parsed.keys())\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(diffs), figsize=(5 * len(diffs), 4), squeeze=False)\n",
        "    axes = axes.reshape(-1)\n",
        "\n",
        "    for ax, dn in zip(axes, diffs):\n",
        "        d = parsed[dn]\n",
        "        ax.plot(d['epoch'], d['train_loss'], marker='o', label='train')\n",
        "        ax.plot(d['epoch'], d['val_loss'], marker='o', label='val')\n",
        "        ax.set_title(dn)\n",
        "        ax.set_xlabel('epoch')\n",
        "        ax.set_ylabel('masked BCE loss')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend()\n",
        "\n",
        "    fig.suptitle('Task 3: train/val loss vs epoch (parsed from my training logs)')\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.90])\n",
        "    fig.savefig(out_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "    print('wrote', out_path)\n",
        "\n",
        "    return parsed\n",
        "\n",
        "\n",
        "parsed = plot_task3_loss_curves_from_logs(TASK3_TRAIN_LOG, out_path=fig_dir / 'task3_loss_curves_from_logs.png')\n",
        "\n",
        "# I print a quick summary like I did for Task 1.\n",
        "for dn in ['easy', 'medium', 'hard']:\n",
        "    if dn not in parsed:\n",
        "        continue\n",
        "    d = parsed[dn]\n",
        "    print(dn, 'epochs:', len(d['epoch']), 'final train loss:', d['train_loss'][-1], 'final val loss:', d['val_loss'][-1])\n",
        "\n",
        "# I also save one small per-difficulty loss curve.\n",
        "for dn in ['easy', 'medium', 'hard']:\n",
        "    if dn not in parsed:\n",
        "        continue\n",
        "    d = parsed[dn]\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.plot(d['epoch'], d['train_loss'], marker='o', label='train')\n",
        "    ax.plot(d['epoch'], d['val_loss'], marker='o', label='val')\n",
        "    ax.set_title(f'Task 3 ({dn}): loss vs epoch')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('masked BCE loss')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    out = fig_dir / f'task3_{dn}_loss_curve.png'\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out, dpi=200)\n",
        "    plt.close(fig)\n",
        "    print('wrote', out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[task3/easy] v2 checkpoint not found locally → generating heatmaps from v1 only\n",
            "[task3/medium] v2 checkpoint not found locally → generating heatmaps from v1 only\n",
            "[task3/hard] v2 checkpoint not found locally → generating heatmaps from v1 only\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/pr/8nr2v4yj5dv82j5lc4nl4h8m0000gn/T/ipykernel_36787/3807624579.py:106: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "  fig.tight_layout(rect=[0, 0, 1, 0.92])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task3_easy_v1_heatmaps_steps_1_to_8.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task3_medium_v1_heatmaps_steps_1_to_8.png\n",
            "wrote /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Deep Learning 01-198-462/minesweeper-dl-submission/docs/figures/task3_hard_v1_heatmaps_steps_1_to_8.png\n"
          ]
        }
      ],
      "source": [
        "# Task 3 — Heatmap evolution (generated here from v1 checkpoints)\n",
        "# I regenerate this figure directly from the v1 `.pt` checkpoints that exist in this repo.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "except Exception as e:\n",
        "    raise RuntimeError('This section requires PyTorch. Install requirements and restart the kernel.') from e\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from minesweeper.game import MinesweeperGame\n",
        "from minesweeper.logic_bot import LogicBot\n",
        "from models.task1.encoding import visible_to_int8\n",
        "from models.task3.model import ThinkingMinePredictor, ThinkingMinePredictorConfig\n",
        "\n",
        "\n",
        "def _make_partially_revealed_board(*, height: int, width: int, num_mines: int, seed: int, click_steps: int = 25):\n",
        "    # I generate one deterministic partially-revealed board so step-by-step heatmaps are comparable.\n",
        "    game = MinesweeperGame(height=int(height), width=int(width), num_mines=int(num_mines), seed=int(seed))\n",
        "    setattr(game, 'allow_mine_triggers', True)\n",
        "\n",
        "    # First click initializes the hidden board and usually opens a region.\n",
        "    r0 = int(height // 2)\n",
        "    c0 = int(width // 2)\n",
        "    _ = game.player_clicks(r0, c0, set())\n",
        "\n",
        "    bot = LogicBot(game, seed=int(seed) + 1337)\n",
        "\n",
        "    clicks = 0\n",
        "    for _ in range(int(click_steps) * 3):\n",
        "        result, action = bot.play_step()\n",
        "\n",
        "        # Count only click actions (flags don’t change the visible board).\n",
        "        if isinstance(action, dict) and action.get('type') != 'flag' and 'pos' in action:\n",
        "            clicks += 1\n",
        "\n",
        "        if result in {'Win', 'Done'}:\n",
        "            break\n",
        "        if clicks >= int(click_steps):\n",
        "            break\n",
        "\n",
        "    return game.get_visible_board()\n",
        "\n",
        "\n",
        "def _load_task3_v1(diff_name: str, *, device: torch.device):\n",
        "    ckpt_path = Path(repo_root) / 'models' / 'task3' / 'checkpoints' / f'task3_{diff_name}_v1_baseline_15ep_loss_heatmap.pt'\n",
        "    if not ckpt_path.exists():\n",
        "        raise FileNotFoundError(f'Missing v1 Task 3 checkpoint: {ckpt_path}')\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    mcfg = ckpt.get('model_cfg') or {}\n",
        "    cfg = ThinkingMinePredictorConfig(**mcfg)\n",
        "    model = ThinkingMinePredictor(cfg).to(device)\n",
        "    model.load_state_dict(ckpt['state_dict'])\n",
        "    model.eval()\n",
        "    return model, cfg, ckpt_path\n",
        "\n",
        "\n",
        "TASK3_PRESETS = {\n",
        "    'easy': {'height': 22, 'width': 22, 'num_mines': 50},\n",
        "    'medium': {'height': 22, 'width': 22, 'num_mines': 80},\n",
        "    'hard': {'height': 22, 'width': 22, 'num_mines': 100},\n",
        "}\n",
        "\n",
        "# I confirm v2 checkpoints are not present (this is why I stick to v1 here).\n",
        "for dn in ['easy', 'medium', 'hard']:\n",
        "    v2_any = list((Path(repo_root) / 'models' / 'task3' / 'checkpoints').glob(f'task3_{dn}_v2*.pt'))\n",
        "    if not v2_any:\n",
        "        print(f'[task3/{dn}] v2 checkpoint not found locally → generating heatmaps from v1 only')\n",
        "\n",
        "\n",
        "def save_task3_v1_heatmap_grid(diff_name: str, *, steps: int = 8, seed: int = 0):\n",
        "    preset = TASK3_PRESETS[diff_name]\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model, cfg, ckpt_path = _load_task3_v1(diff_name, device=device)\n",
        "\n",
        "    # One fixed partially revealed board.\n",
        "    visible = _make_partially_revealed_board(\n",
        "        height=int(preset['height']),\n",
        "        width=int(preset['width']),\n",
        "        num_mines=int(preset['num_mines']),\n",
        "        seed=int(seed),\n",
        "        click_steps=25,\n",
        "    )\n",
        "\n",
        "    x = visible_to_int8(visible)  # (H,W) in [-1..9]\n",
        "    xt = torch.from_numpy(x).to(device=device).to(torch.int64).unsqueeze(0)\n",
        "\n",
        "    _, per_step = model(xt, steps=int(steps), return_all=True)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
        "    axes = axes.reshape(-1)\n",
        "    for i in range(8):\n",
        "        probs = torch.sigmoid(per_step[i]).squeeze(0).detach().cpu().numpy()\n",
        "        ax = axes[i]\n",
        "        im = ax.imshow(probs, vmin=0.0, vmax=1.0, cmap='viridis')\n",
        "        ax.set_title(f'step {i+1}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    fig.colorbar(im, ax=axes.tolist(), fraction=0.02, pad=0.02)\n",
        "    fig.suptitle(f'Task 3 ({diff_name}, v1): heatmaps as I let the model think longer\\nckpt={ckpt_path.name}')\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.92])\n",
        "\n",
        "    out = fig_dir / f'task3_{diff_name}_v1_heatmaps_steps_1_to_8.png'\n",
        "    fig.savefig(out, dpi=200)\n",
        "    plt.close(fig)\n",
        "    print('wrote', out)\n",
        "\n",
        "\n",
        "for dn in ['easy', 'medium', 'hard']:\n",
        "    save_task3_v1_heatmap_grid(dn, steps=8, seed=0)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.10.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
