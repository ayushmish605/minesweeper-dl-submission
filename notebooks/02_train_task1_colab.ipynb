{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 02 — Task 1 (Playing by Mine Prediction)\n",
        "\n",
        "In this notebook, I train **three separate Task‑1 models** (one per difficulty):\n",
        "\n",
        "- Easy: 22×22, 50 mines\n",
        "- Medium: 22×22, 80 mines\n",
        "- Hard: 22×22, 100 mines\n",
        "\n",
        "I’m not mounting Google Drive here (it kept hanging for me). I unzip my project bundle so the repo lives at `/content/repo/`, and then I train.\n",
        "\n",
        "I save:\n",
        "- datasets as `.npz` (under `models/task1/datasets/`)\n",
        "- checkpoints as `models/task1/checkpoints/task1_{easy,medium,hard}.pt`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab installs\n",
        "# NOTE: I avoid re-installing torch in Colab because it can create checkpoint loading issues\n",
        "# if the runtime's torch version changes mid-session.\n",
        "%pip install -q numpy matplotlib tqdm scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set repo root (already unzipped under /content/repo)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path('/content/repo')\n",
        "\n",
        "# Sometimes zip extraction creates one extra top-level folder; if so, I step into it.\n",
        "if not ((repo_root / 'minesweeper').exists() and (repo_root / 'models').exists()):\n",
        "    kids = [p for p in repo_root.iterdir() if p.is_dir()]\n",
        "    if len(kids) == 1:\n",
        "        repo_root = kids[0]\n",
        "\n",
        "if not ((repo_root / 'minesweeper').exists() and (repo_root / 'models').exists()):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Bad repo_root: {repo_root}\\n\"\n",
        "        \"I expected `minesweeper/` and `models/` directly inside /content/repo.\"\n",
        "    )\n",
        "\n",
        "sys.path.insert(0, str(repo_root))\n",
        "print(f\"Repo root: {repo_root}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Generate / load datasets\n",
        "\n",
        "I generate one `.npz` dataset per difficulty. I keep the datasets under `models/task1/datasets/` so I don’t have to regenerate them every run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from models.dataset_cache import dataset_dir_for_task, ensure_npz\n",
        "from models.task1.dataset import generate_task1_dataset_npz\n",
        "from models.task1 import load_task1_npz\n",
        "\n",
        "# I store datasets under models/task1/datasets/ so they travel with the task code.\n",
        "DATA_DIR = dataset_dir_for_task(repo_root=repo_root, task='task1')\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DIFFICULTIES = {\n",
        "    'easy': {'height': 22, 'width': 22, 'num_mines': 50},\n",
        "    'medium': {'height': 22, 'width': 22, 'num_mines': 80},\n",
        "    'hard': {'height': 22, 'width': 22, 'num_mines': 100},\n",
        "}\n",
        "\n",
        "# I can scale these up for better generalization.\n",
        "NUM_GAMES = {'easy': 1000, 'medium': 2000, 'hard': 3000}\n",
        "\n",
        "datasets = {}\n",
        "for name, cfg in DIFFICULTIES.items():\n",
        "    out_npz = DATA_DIR / f'task1_{name}_teacher_logic.npz'\n",
        "\n",
        "    ensure_npz(\n",
        "        out_path=out_npz,\n",
        "        generator=generate_task1_dataset_npz,\n",
        "        generator_kwargs=dict(\n",
        "            height=cfg['height'],\n",
        "            width=cfg['width'],\n",
        "            num_mines=cfg['num_mines'],\n",
        "            num_games=NUM_GAMES[name],\n",
        "            teacher='logic',\n",
        "            allow_mine_triggers=True,\n",
        "            max_clicks_per_game=512,\n",
        "            seed=0,\n",
        "        ),\n",
        "        force=False,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    npz = load_task1_npz(out_npz)\n",
        "    meta = json.loads(npz.meta_json)\n",
        "    print(f\"{name}: samples={npz.x_visible.shape[0]} games={meta.get('num_games')}\")\n",
        "    datasets[name] = npz\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Train models (masked loss)\n",
        "\n",
        "I train on unrevealed cells only (that’s where the uncertainty is).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I keep the training loop pretty straightforward:\n",
        "\n",
        "- model outputs per-cell mine logits\n",
        "- loss is masked so I only train on unrevealed cells\n",
        "- I train one model per difficulty and save checkpoints under `models/task1/checkpoints/`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import asdict\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from models.metrics import (\n",
        "    masked_bce_with_logits,\n",
        "    masked_binary_confusion_from_logits,\n",
        "    binary_metrics_from_confusion,\n",
        "    pos_weight_from_targets,\n",
        ")\n",
        "from models.task1 import Task1Dataset\n",
        "from models.task1.model import MinePredictor, MinePredictorConfig\n",
        "\n",
        "\n",
        "def _add_conf(dst: dict, src: dict) -> None:\n",
        "    for k in ('tp', 'fp', 'tn', 'fn', 'n'):\n",
        "        dst[k] = int(dst.get(k, 0) or 0) + int(src.get(k, 0) or 0)\n",
        "\n",
        "\n",
        "def train_one_model(\n",
        "    *,\n",
        "    name: str,\n",
        "    npz,\n",
        "    cfg: MinePredictorConfig,\n",
        "    epochs: int = 15,\n",
        "    batch_size: int = 64,\n",
        "    lr: float = 3e-4,\n",
        "    weight_decay: float = 1e-2,\n",
        "    val_frac: float = 0.1,\n",
        "    seed: int = 0,\n",
        "    threshold: float = 0.5,\n",
        "    use_pos_weight: bool = True,\n",
        "    early_stop_patience: int = 4,\n",
        "    early_stop_min_delta: float = 1e-4,\n",
        "):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    ds = Task1Dataset(npz)\n",
        "    g = torch.Generator().manual_seed(seed)\n",
        "    perm = torch.randperm(len(ds), generator=g)\n",
        "    n_val = int(len(ds) * val_frac)\n",
        "    val_idx = perm[:n_val].tolist()\n",
        "    train_idx = perm[n_val:].tolist()\n",
        "\n",
        "    train_loader = DataLoader(torch.utils.data.Subset(ds, train_idx), batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(torch.utils.data.Subset(ds, val_idx), batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = MinePredictor(cfg).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    use_amp = torch.cuda.is_available()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(1, int(epochs) + 1):\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        tr_steps = 0\n",
        "        tr_conf = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0, 'n': 0}\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x = batch['x'].to(device)\n",
        "            y = batch['y'].to(device)\n",
        "            m = batch['mask'].to(device)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                logits = model(x)\n",
        "                pw = pos_weight_from_targets(y, m) if bool(use_pos_weight) else None\n",
        "                loss = masked_bce_with_logits(logits, y, m, pos_weight=pw)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "            tr_loss += float(loss.item())\n",
        "            tr_steps += 1\n",
        "            _add_conf(tr_conf, masked_binary_confusion_from_logits(logits.detach(), y, m, threshold=float(threshold)))\n",
        "\n",
        "        tr_loss /= max(1, tr_steps)\n",
        "        tr_m = binary_metrics_from_confusion(tr_conf['tp'], tr_conf['fp'], tr_conf['tn'], tr_conf['fn'])\n",
        "\n",
        "        model.eval()\n",
        "        va_loss = 0.0\n",
        "        va_steps = 0\n",
        "        va_conf = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0, 'n': 0}\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x = batch['x'].to(device)\n",
        "                y = batch['y'].to(device)\n",
        "                m = batch['mask'].to(device)\n",
        "                logits = model(x)\n",
        "                pw = pos_weight_from_targets(y, m) if bool(use_pos_weight) else None\n",
        "                va_loss += float(masked_bce_with_logits(logits, y, m, pos_weight=pw).item())\n",
        "                va_steps += 1\n",
        "                _add_conf(va_conf, masked_binary_confusion_from_logits(logits, y, m, threshold=float(threshold)))\n",
        "\n",
        "        va_loss /= max(1, va_steps)\n",
        "        va_m = binary_metrics_from_confusion(va_conf['tp'], va_conf['fp'], va_conf['tn'], va_conf['fn'])\n",
        "\n",
        "        print(\n",
        "            f'[{name}] epoch {epoch}/{epochs} | '\n",
        "            f'train loss {tr_loss:.4f} acc {tr_m[\"acc\"]:.3f} prec {tr_m[\"precision\"]:.3f} rec {tr_m[\"recall\"]:.3f} f1 {tr_m[\"f1\"]:.3f} | '\n",
        "            f'val loss {va_loss:.4f} acc {va_m[\"acc\"]:.3f} prec {va_m[\"precision\"]:.3f} rec {va_m[\"recall\"]:.3f} f1 {va_m[\"f1\"]:.3f}'\n",
        "        )\n",
        "\n",
        "        cur_f1 = float(va_m.get('f1', 0.0) or 0.0)\n",
        "        if cur_f1 > (best_f1 + float(early_stop_min_delta)):\n",
        "            best_f1 = cur_f1\n",
        "            best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= int(early_stop_patience):\n",
        "                print(f'[{name}] early stop: no val f1 improvement for {early_stop_patience} epoch(s). best_f1={best_f1:.4f}')\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from dataclasses import asdict\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from models.task1.model import MinePredictorConfig\n",
        "\n",
        "CKPT_DIR = Path(repo_root) / 'models' / 'task1' / 'checkpoints'\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAINING_CFG = dict(epochs=15, batch_size=64, lr=3e-4, weight_decay=1e-2, val_frac=0.1, seed=0, early_stop_patience=4)\n",
        "\n",
        "trained = {}\n",
        "for name, diff in DIFFICULTIES.items():\n",
        "    cfg = MinePredictorConfig(height=diff['height'], width=diff['width'])\n",
        "    model = train_one_model(name=name, npz=datasets[name], cfg=cfg, **TRAINING_CFG)\n",
        "\n",
        "    ckpt_path = CKPT_DIR / f'task1_{name}.pt'\n",
        "    meta = json.loads(datasets[name].meta_json)\n",
        "\n",
        "    torch.save(\n",
        "        {\n",
        "            'task': 'task1_mine_prediction',\n",
        "            'difficulty': name,\n",
        "            'difficulty_cfg': diff,\n",
        "            'dataset_meta': meta,\n",
        "            'model_cfg': asdict(cfg),\n",
        "            'state_dict': model.state_dict(),\n",
        "        },\n",
        "        ckpt_path,\n",
        "    )\n",
        "    print(f'Saved {name} -> {ckpt_path}')\n",
        "    trained[name] = str(ckpt_path)\n",
        "\n",
        "trained\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
